{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mHtzAG5_c-Z",
        "outputId": "ac5f4252-f012-4bd8-f537-9f5621f4be30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "oTxT_zym_sOJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/ckd.csv')"
      ],
      "metadata": {
        "id": "tN4tyky-_-gH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "k8xBdX9VAM3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c84077de-e56a-4471-f660-3a527a4f9bd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id   age    bp     sg   al   su     rbc        pc         pcc  \\\n",
              "0      0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent   \n",
              "1      1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent   \n",
              "2      2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent   \n",
              "3      3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present   \n",
              "4      4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent   \n",
              "..   ...   ...   ...    ...  ...  ...     ...       ...         ...   \n",
              "395  395  55.0  80.0  1.020  0.0  0.0  normal    normal  notpresent   \n",
              "396  396  42.0  70.0  1.025  0.0  0.0  normal    normal  notpresent   \n",
              "397  397  12.0  80.0  1.020  0.0  0.0  normal    normal  notpresent   \n",
              "398  398  17.0  60.0  1.025  0.0  0.0  normal    normal  notpresent   \n",
              "399  399  58.0  80.0  1.025  0.0  0.0  normal    normal  notpresent   \n",
              "\n",
              "             ba  ...  pcv    wc   rc  htn   dm  cad appet   pe  ane  \\\n",
              "0    notpresent  ...   44  7800  5.2  yes  yes   no  good   no   no   \n",
              "1    notpresent  ...   38  6000  NaN   no   no   no  good   no   no   \n",
              "2    notpresent  ...   31  7500  NaN   no  yes   no  poor   no  yes   \n",
              "3    notpresent  ...   32  6700  3.9  yes   no   no  poor  yes  yes   \n",
              "4    notpresent  ...   35  7300  4.6   no   no   no  good   no   no   \n",
              "..          ...  ...  ...   ...  ...  ...  ...  ...   ...  ...  ...   \n",
              "395  notpresent  ...   47  6700  4.9   no   no   no  good   no   no   \n",
              "396  notpresent  ...   54  7800  6.2   no   no   no  good   no   no   \n",
              "397  notpresent  ...   49  6600  5.4   no   no   no  good   no   no   \n",
              "398  notpresent  ...   51  7200  5.9   no   no   no  good   no   no   \n",
              "399  notpresent  ...   53  6800  6.1   no   no   no  good   no   no   \n",
              "\n",
              "    classification  \n",
              "0              ckd  \n",
              "1              ckd  \n",
              "2              ckd  \n",
              "3              ckd  \n",
              "4              ckd  \n",
              "..             ...  \n",
              "395         notckd  \n",
              "396         notckd  \n",
              "397         notckd  \n",
              "398         notckd  \n",
              "399         notckd  \n",
              "\n",
              "[400 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c64087a5-868c-4c4f-bae8-1932659b728a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>bp</th>\n",
              "      <th>sg</th>\n",
              "      <th>al</th>\n",
              "      <th>su</th>\n",
              "      <th>rbc</th>\n",
              "      <th>pc</th>\n",
              "      <th>pcc</th>\n",
              "      <th>ba</th>\n",
              "      <th>...</th>\n",
              "      <th>pcv</th>\n",
              "      <th>wc</th>\n",
              "      <th>rc</th>\n",
              "      <th>htn</th>\n",
              "      <th>dm</th>\n",
              "      <th>cad</th>\n",
              "      <th>appet</th>\n",
              "      <th>pe</th>\n",
              "      <th>ane</th>\n",
              "      <th>classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>7800</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>31</td>\n",
              "      <td>7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>48.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>6700</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>51.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>35</td>\n",
              "      <td>7300</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>395</td>\n",
              "      <td>55.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>47</td>\n",
              "      <td>6700</td>\n",
              "      <td>4.9</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>notckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>396</td>\n",
              "      <td>42.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>7800</td>\n",
              "      <td>6.2</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>notckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>397</td>\n",
              "      <td>12.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>49</td>\n",
              "      <td>6600</td>\n",
              "      <td>5.4</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>notckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>398</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>1.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>51</td>\n",
              "      <td>7200</td>\n",
              "      <td>5.9</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>notckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>399</td>\n",
              "      <td>58.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>...</td>\n",
              "      <td>53</td>\n",
              "      <td>6800</td>\n",
              "      <td>6.1</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>notckd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c64087a5-868c-4c4f-bae8-1932659b728a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c64087a5-868c-4c4f-bae8-1932659b728a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c64087a5-868c-4c4f-bae8-1932659b728a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "lSTFRj7wBVjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b405317-d8a3-4971-b137-bb91b328bc7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 26 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   id              400 non-null    int64  \n",
            " 1   age             391 non-null    float64\n",
            " 2   bp              388 non-null    float64\n",
            " 3   sg              353 non-null    float64\n",
            " 4   al              354 non-null    float64\n",
            " 5   su              351 non-null    float64\n",
            " 6   rbc             248 non-null    object \n",
            " 7   pc              335 non-null    object \n",
            " 8   pcc             396 non-null    object \n",
            " 9   ba              396 non-null    object \n",
            " 10  bgr             356 non-null    float64\n",
            " 11  bu              381 non-null    float64\n",
            " 12  sc              383 non-null    float64\n",
            " 13  sod             313 non-null    float64\n",
            " 14  pot             312 non-null    float64\n",
            " 15  hemo            348 non-null    float64\n",
            " 16  pcv             330 non-null    object \n",
            " 17  wc              295 non-null    object \n",
            " 18  rc              270 non-null    object \n",
            " 19  htn             398 non-null    object \n",
            " 20  dm              398 non-null    object \n",
            " 21  cad             398 non-null    object \n",
            " 22  appet           399 non-null    object \n",
            " 23  pe              399 non-null    object \n",
            " 24  ane             399 non-null    object \n",
            " 25  classification  400 non-null    object \n",
            "dtypes: float64(11), int64(1), object(14)\n",
            "memory usage: 81.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    print(f\"{col} has {df[col].unique()} values\\n\")"
      ],
      "metadata": {
        "id": "nhNH2lZafNKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af16a98f-01e5-4153-e4f0-1fb293da346c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id has [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399] values\n",
            "\n",
            "age has [48.  7. 62. 51. 60. 68. 24. 52. 53. 50. 63. 40. 47. 61. 21. 42. 75. 69.\n",
            " nan 73. 70. 65. 76. 72. 82. 46. 45. 35. 54. 11. 59. 67. 15. 55. 44. 26.\n",
            " 64. 56.  5. 74. 38. 58. 71. 34. 17. 12. 43. 41. 57.  8. 39. 66. 81. 14.\n",
            " 27. 83. 30.  4.  3.  6. 32. 80. 49. 90. 78. 19.  2. 33. 36. 37. 23. 25.\n",
            " 20. 29. 28. 22. 79.] values\n",
            "\n",
            "bp has [ 80.  50.  70.  90.  nan 100.  60. 110. 140. 180. 120.] values\n",
            "\n",
            "sg has [1.02  1.01  1.005 1.015   nan 1.025] values\n",
            "\n",
            "al has [ 1.  4.  2.  3.  0. nan  5.] values\n",
            "\n",
            "su has [ 0.  3.  4.  1. nan  2.  5.] values\n",
            "\n",
            "rbc has [nan 'normal' 'abnormal'] values\n",
            "\n",
            "pc has ['normal' 'abnormal' nan] values\n",
            "\n",
            "pcc has ['notpresent' 'present' nan] values\n",
            "\n",
            "ba has ['notpresent' 'present' nan] values\n",
            "\n",
            "bgr has [121.  nan 423. 117. 106.  74. 100. 410. 138.  70. 490. 380. 208.  98.\n",
            " 157.  76.  99. 114. 263. 173.  95. 108. 156. 264. 123.  93. 107. 159.\n",
            " 140. 171. 270.  92. 137. 204.  79. 207. 124. 144.  91. 162. 246. 253.\n",
            " 141. 182.  86. 150. 146. 425. 112. 250. 360. 163. 129. 133. 102. 158.\n",
            " 165. 132. 104. 127. 415. 169. 251. 109. 280. 210. 219. 295.  94. 172.\n",
            " 101. 298. 153.  88. 226. 143. 115.  89. 297. 233. 294. 323. 125.  90.\n",
            " 308. 118. 224. 128. 122. 214. 213. 268. 256.  84. 105. 288. 139.  78.\n",
            " 273. 242. 424. 303. 148. 160. 192. 307. 220. 447. 309.  22. 111. 261.\n",
            " 215. 234. 131. 352.  80. 239. 110. 130. 184. 252. 113. 230. 341. 255.\n",
            " 103. 238. 248. 120. 241. 269. 201. 203. 463. 176.  82. 119.  97.  96.\n",
            "  81. 116. 134.  85.  83.  87.  75.] values\n",
            "\n",
            "bu has [ 36.   18.   53.   56.   26.   25.   54.   31.   60.  107.   55.   72.\n",
            "  86.   90.  162.   46.   87.   27.  148.  180.  163.    nan  50.   75.\n",
            "  45.   28.  155.   33.   39.  153.   29.   65.  103.   70.   80.   20.\n",
            " 202.   77.   89.   24.   17.   32.  114.   66.   38.  164.  142.   96.\n",
            " 391.   15.  111.   73.   19.   92.   35.   16.  139.   48.   85.   98.\n",
            " 186.   37.   47.   52.   82.   51.  106.   22.  217.   88.  118.   50.1\n",
            "  71.   34.   40.   21.  219.   30.  125.  166.   49.  208.  176.   68.\n",
            " 145.  165.  322.   23.  235.  132.   76.   42.   44.   41.  113.    1.5\n",
            " 146.   58.  133.  137.   67.  115.  223.   98.6 158.   94.   74.  150.\n",
            "  61.   57.   95.  191.   93.  241.   64.   79.  215.  309.   10. ] values\n",
            "\n",
            "sc has [ 1.2   0.8   1.8   3.8   1.4   1.1  24.    1.9   7.2   4.    2.7   2.1\n",
            "  4.6   4.1   9.6   2.2   5.2   1.3   1.6   3.9  76.    7.7    nan  2.4\n",
            "  7.3   1.5   2.5   2.    3.4   0.7   1.   10.8   6.3   5.9   0.9   3.\n",
            "  3.25  9.7   6.4   3.2  32.    0.6   6.1   3.3   6.7   8.5   2.8  15.\n",
            "  2.9   1.7   3.6   5.6   6.5   4.4  10.2  11.5   0.5  12.2   5.3   9.2\n",
            " 13.8  16.9   6.    7.1  18.    2.3  13.   48.1  14.2  16.4   2.6   7.5\n",
            "  4.3  18.1  11.8   9.3   6.8  13.5  12.8  11.9  12.   13.4  15.2  13.3\n",
            "  0.4 ] values\n",
            "\n",
            "sod has [  nan 111.  142.  104.  114.  131.  138.  135.  130.  141.  139.    4.5\n",
            " 136.  129.  140.  132.  133.  134.  125.  163.  137.  128.  143.  127.\n",
            " 146.  126.  122.  147.  124.  115.  145.  113.  120.  150.  144. ] values\n",
            "\n",
            "pot has [ nan  2.5  3.2  4.   3.7  4.2  5.8  3.4  6.4  4.9  4.1  4.3  5.2  3.8\n",
            "  4.6  3.9  4.7  5.9  4.8  4.4  6.6 39.   5.5  5.   3.5  3.6  7.6  2.9\n",
            "  4.5  5.7  5.4  5.3 47.   6.3  5.1  5.6  3.   2.8  2.7  6.5  3.3] values\n",
            "\n",
            "hemo has [15.4 11.3  9.6 11.2 11.6 12.2 12.4 10.8  9.5  9.4  9.7  9.8  5.6  7.6\n",
            " 12.6 12.1 12.7 10.3  7.7 10.9  nan 11.1  9.9 12.5 12.9 10.1 12.  13.\n",
            "  7.9  9.3 15.  10.   8.6 13.6 10.2 10.5  6.6 11.   7.5 15.6 15.2  4.8\n",
            "  9.1  8.1 11.9 13.5  8.3  7.1 16.1 10.4  9.2  6.2 13.9 14.1  6.  11.8\n",
            " 11.7 11.4 14.   8.2 13.2  6.1  8.  12.3  8.4 14.3  9.   8.7 10.6 13.1\n",
            " 10.7  5.5  5.8  6.8  8.8  8.5 13.8 11.5  7.3 13.7 12.8 13.4  6.3  3.1\n",
            " 17.  15.9 14.5 15.5 16.2 14.4 14.2 16.3 14.8 16.5 15.7 13.3 14.6 16.4\n",
            " 16.9 16.  14.7 16.6 14.9 16.7 16.8 15.8 15.1 17.1 17.2 15.3 17.3 17.4\n",
            " 17.7 17.8 17.5 17.6] values\n",
            "\n",
            "pcv has ['44' '38' '31' '32' '35' '39' '36' '33' '29' '28' nan '16' '24' '37' '30'\n",
            " '34' '40' '45' '27' '48' '\\t?' '52' '14' '22' '18' '42' '17' '46' '23'\n",
            " '19' '25' '41' '26' '15' '21' '43' '20' '\\t43' '47' '9' '49' '50' '53'\n",
            " '51' '54'] values\n",
            "\n",
            "wc has ['7800' '6000' '7500' '6700' '7300' nan '6900' '9600' '12100' '4500'\n",
            " '12200' '11000' '3800' '11400' '5300' '9200' '6200' '8300' '8400' '10300'\n",
            " '9800' '9100' '7900' '6400' '8600' '18900' '21600' '4300' '8500' '11300'\n",
            " '7200' '7700' '14600' '6300' '\\t6200' '7100' '11800' '9400' '5500' '5800'\n",
            " '13200' '12500' '5600' '7000' '11900' '10400' '10700' '12700' '6800'\n",
            " '6500' '13600' '10200' '9000' '14900' '8200' '15200' '5000' '16300'\n",
            " '12400' '\\t8400' '10500' '4200' '4700' '10900' '8100' '9500' '2200'\n",
            " '12800' '11200' '19100' '\\t?' '12300' '16700' '2600' '26400' '8800'\n",
            " '7400' '4900' '8000' '12000' '15700' '4100' '5700' '11500' '5400' '10800'\n",
            " '9900' '5200' '5900' '9300' '9700' '5100' '6600'] values\n",
            "\n",
            "rc has ['5.2' nan '3.9' '4.6' '4.4' '5' '4.0' '3.7' '3.8' '3.4' '2.6' '2.8' '4.3'\n",
            " '3.2' '3.6' '4' '4.1' '4.9' '2.5' '4.2' '4.5' '3.1' '4.7' '3.5' '6.0'\n",
            " '5.0' '2.1' '5.6' '2.3' '2.9' '2.7' '8.0' '3.3' '3.0' '3' '2.4' '4.8'\n",
            " '\\t?' '5.4' '6.1' '6.2' '6.3' '5.1' '5.8' '5.5' '5.3' '6.4' '5.7' '5.9'\n",
            " '6.5'] values\n",
            "\n",
            "htn has ['yes' 'no' nan] values\n",
            "\n",
            "dm has ['yes' 'no' ' yes' '\\tno' '\\tyes' nan] values\n",
            "\n",
            "cad has ['no' 'yes' '\\tno' nan] values\n",
            "\n",
            "appet has ['good' 'poor' nan] values\n",
            "\n",
            "pe has ['no' 'yes' nan] values\n",
            "\n",
            "ane has ['no' 'yes' nan] values\n",
            "\n",
            "classification has ['ckd' 'ckd\\t' 'notckd'] values\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.classification.unique()"
      ],
      "metadata": {
        "id": "QA6bJ8ruFePG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6281554f-2c0f-4a11-d2ef-47c8a686aa2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ckd', 'ckd\\t', 'notckd'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.classification=df.classification.replace(\"ckd\\t\",\"ckd\")\n",
        "df.classification.unique()"
      ],
      "metadata": {
        "id": "iGZFZAxRFrN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455148f4-d53d-416f-efab-2b899d90974a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ckd', 'notckd'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(\"id\",axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "BR_VY0F1GjXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b89b165-9908-425e-f2af-b23112ba88c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['classification']=df['classification'].replace(['ckd',\"notckd\"],[1,0])\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "sH9Cvzt-HB18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f7110907-3ec1-4760-f21f-cedbea517961"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
              "0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
              "1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
              "2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
              "3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
              "4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
              "\n",
              "     bgr  ...  pcv    wc   rc  htn   dm cad appet   pe  ane classification  \n",
              "0  121.0  ...   44  7800  5.2  yes  yes  no  good   no   no              1  \n",
              "1    NaN  ...   38  6000  NaN   no   no  no  good   no   no              1  \n",
              "2  423.0  ...   31  7500  NaN   no  yes  no  poor   no  yes              1  \n",
              "3  117.0  ...   32  6700  3.9  yes   no  no  poor  yes  yes              1  \n",
              "4  106.0  ...   35  7300  4.6   no   no  no  good   no   no              1  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87abe618-2ae3-414a-813e-e08a34c46eb4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bp</th>\n",
              "      <th>sg</th>\n",
              "      <th>al</th>\n",
              "      <th>su</th>\n",
              "      <th>rbc</th>\n",
              "      <th>pc</th>\n",
              "      <th>pcc</th>\n",
              "      <th>ba</th>\n",
              "      <th>bgr</th>\n",
              "      <th>...</th>\n",
              "      <th>pcv</th>\n",
              "      <th>wc</th>\n",
              "      <th>rc</th>\n",
              "      <th>htn</th>\n",
              "      <th>dm</th>\n",
              "      <th>cad</th>\n",
              "      <th>appet</th>\n",
              "      <th>pe</th>\n",
              "      <th>ane</th>\n",
              "      <th>classification</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>7800</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>6000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>423.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31</td>\n",
              "      <td>7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>6700</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>106.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35</td>\n",
              "      <td>7300</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87abe618-2ae3-414a-813e-e08a34c46eb4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87abe618-2ae3-414a-813e-e08a34c46eb4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87abe618-2ae3-414a-813e-e08a34c46eb4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['age', 'blood_pressure', 'specific_gravity', 'albumin', 'sugar', 'red_blood_cells', 'pus_cell',\n",
        "              'pus_cell_clumps', 'bacteria', 'blood_glucose_random', 'blood_urea', 'serum_creatinine', 'sodium',\n",
        "              'potassium', 'haemoglobin', 'packed_cell_volume', 'white_blood_cell_count', 'red_blood_cell_count',\n",
        "              'hypertension', 'diabetes_mellitus', 'coronary_artery_disease', 'appetite', 'peda_edema',\n",
        "              'aanemia', 'class']\n",
        "              "
      ],
      "metadata": {
        "id": "gfLQ_5omZdvk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['packed_cell_volume'] = pd.to_numeric(df['packed_cell_volume'], errors='coerce')\n",
        "df['white_blood_cell_count'] = pd.to_numeric(df['white_blood_cell_count'], errors='coerce')\n",
        "df['red_blood_cell_count'] = pd.to_numeric(df['red_blood_cell_count'], errors='coerce')"
      ],
      "metadata": {
        "id": "CgDWjXKjbTJo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
        "num_cols = [col for col in df.columns if df[col].dtype != 'object']"
      ],
      "metadata": {
        "id": "TrojdOnreD5N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in cat_cols:\n",
        "    print(f\"{col} has {df[col].unique()} values\\n\")"
      ],
      "metadata": {
        "id": "WQ7nafkMeLOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8935b48-dba2-451c-bb2a-eeec2b8137ab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red_blood_cells has [nan 'normal' 'abnormal'] values\n",
            "\n",
            "pus_cell has ['normal' 'abnormal' nan] values\n",
            "\n",
            "pus_cell_clumps has ['notpresent' 'present' nan] values\n",
            "\n",
            "bacteria has ['notpresent' 'present' nan] values\n",
            "\n",
            "hypertension has ['yes' 'no' nan] values\n",
            "\n",
            "diabetes_mellitus has ['yes' 'no' ' yes' '\\tno' '\\tyes' nan] values\n",
            "\n",
            "coronary_artery_disease has ['no' 'yes' '\\tno' nan] values\n",
            "\n",
            "appetite has ['good' 'poor' nan] values\n",
            "\n",
            "peda_edema has ['no' 'yes' nan] values\n",
            "\n",
            "aanemia has ['no' 'yes' nan] values\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['diabetes_mellitus'].replace(to_replace = {'\\tno':'no','\\tyes':'yes',' yes':'yes'},inplace=True)\n",
        "\n",
        "df['coronary_artery_disease'] = df['coronary_artery_disease'].replace(to_replace = '\\tno', value='no')\n"
      ],
      "metadata": {
        "id": "AMfJj_EyfrMB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "RQRfbyUHgLJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd16245-de42-406c-e863-782d7e23338f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "red_blood_cells            152\n",
              "red_blood_cell_count       131\n",
              "white_blood_cell_count     106\n",
              "potassium                   88\n",
              "sodium                      87\n",
              "packed_cell_volume          71\n",
              "pus_cell                    65\n",
              "haemoglobin                 52\n",
              "sugar                       49\n",
              "specific_gravity            47\n",
              "albumin                     46\n",
              "blood_glucose_random        44\n",
              "blood_urea                  19\n",
              "serum_creatinine            17\n",
              "blood_pressure              12\n",
              "age                          9\n",
              "bacteria                     4\n",
              "pus_cell_clumps              4\n",
              "hypertension                 2\n",
              "diabetes_mellitus            2\n",
              "coronary_artery_disease      2\n",
              "appetite                     1\n",
              "peda_edema                   1\n",
              "aanemia                      1\n",
              "class                        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[num_cols].isnull().sum()"
      ],
      "metadata": {
        "id": "GGcmnE6Zggvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dd42c2-4f83-4ada-8ded-b56a62aae722"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                         9\n",
              "blood_pressure             12\n",
              "specific_gravity           47\n",
              "albumin                    46\n",
              "sugar                      49\n",
              "blood_glucose_random       44\n",
              "blood_urea                 19\n",
              "serum_creatinine           17\n",
              "sodium                     87\n",
              "potassium                  88\n",
              "haemoglobin                52\n",
              "packed_cell_volume         71\n",
              "white_blood_cell_count    106\n",
              "red_blood_cell_count      131\n",
              "class                       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[cat_cols].isnull().sum()"
      ],
      "metadata": {
        "id": "FRdvwybggnLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008d0741-5997-4bac-ea2a-af52aadafc64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "red_blood_cells            152\n",
              "pus_cell                    65\n",
              "pus_cell_clumps              4\n",
              "bacteria                     4\n",
              "hypertension                 2\n",
              "diabetes_mellitus            2\n",
              "coronary_artery_disease      2\n",
              "appetite                     1\n",
              "peda_edema                   1\n",
              "aanemia                      1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_value_imputation(feature):\n",
        "    random_sample = df[feature].dropna().sample(df[feature].isna().sum())\n",
        "    print(random_sample)\n",
        "    random_sample.index = df[df[feature].isnull()].index\n",
        "    print(random_sample.index)\n",
        "    df.loc[df[feature].isnull(), feature] = random_sample\n",
        "\n",
        "for col in num_cols:\n",
        "    random_value_imputation(col)\n",
        "\n",
        "random_value_imputation('red_blood_cells')\n",
        "random_value_imputation('pus_cell')    "
      ],
      "metadata": {
        "id": "aE7LTlAE4nMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98e6634-1cac-49c0-aaf4-022670dbadc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "328    28.0\n",
            "280    47.0\n",
            "44     54.0\n",
            "214    68.0\n",
            "380    59.0\n",
            "204    65.0\n",
            "341    63.0\n",
            "325    58.0\n",
            "111    65.0\n",
            "Name: age, dtype: float64\n",
            "Int64Index([30, 73, 112, 116, 117, 169, 191, 203, 268], dtype='int64')\n",
            "165     80.0\n",
            "183     70.0\n",
            "18     100.0\n",
            "17      80.0\n",
            "296     70.0\n",
            "263     80.0\n",
            "292     80.0\n",
            "160     60.0\n",
            "59     100.0\n",
            "364     80.0\n",
            "245    100.0\n",
            "140     70.0\n",
            "Name: blood_pressure, dtype: float64\n",
            "Int64Index([7, 75, 132, 138, 161, 164, 185, 187, 188, 215, 293, 316], dtype='int64')\n",
            "253    1.025\n",
            "187    1.010\n",
            "335    1.020\n",
            "211    1.015\n",
            "235    1.010\n",
            "327    1.025\n",
            "55     1.005\n",
            "138    1.010\n",
            "194    1.010\n",
            "265    1.020\n",
            "250    1.025\n",
            "7      1.015\n",
            "60     1.020\n",
            "70     1.015\n",
            "398    1.025\n",
            "186    1.020\n",
            "226    1.015\n",
            "308    1.025\n",
            "311    1.025\n",
            "206    1.010\n",
            "22     1.025\n",
            "181    1.025\n",
            "307    1.020\n",
            "32     1.010\n",
            "185    1.020\n",
            "246    1.015\n",
            "178    1.020\n",
            "370    1.020\n",
            "190    1.010\n",
            "77     1.010\n",
            "278    1.020\n",
            "71     1.010\n",
            "324    1.020\n",
            "1      1.020\n",
            "263    1.020\n",
            "341    1.025\n",
            "94     1.010\n",
            "348    1.020\n",
            "136    1.020\n",
            "72     1.010\n",
            "365    1.020\n",
            "158    1.020\n",
            "111    1.010\n",
            "241    1.015\n",
            "29     1.005\n",
            "316    1.020\n",
            "127    1.015\n",
            "Name: specific_gravity, dtype: float64\n",
            "Int64Index([ 13,  17,  21,  28,  30,  37,  50,  57,  59,  78,  81,  82,  86,\n",
            "             98,  99, 104, 106, 109, 121, 122, 125, 132, 142, 148, 151, 160,\n",
            "            162, 166, 174, 188, 197, 201, 202, 203, 205, 208, 222, 228, 231,\n",
            "            236, 238, 245, 268, 280, 295, 322, 346],\n",
            "           dtype='int64')\n",
            "319    0.0\n",
            "179    2.0\n",
            "90     2.0\n",
            "207    0.0\n",
            "73     2.0\n",
            "394    0.0\n",
            "83     1.0\n",
            "381    0.0\n",
            "168    4.0\n",
            "273    0.0\n",
            "390    0.0\n",
            "80     0.0\n",
            "359    0.0\n",
            "290    0.0\n",
            "212    3.0\n",
            "298    0.0\n",
            "358    0.0\n",
            "351    0.0\n",
            "183    0.0\n",
            "348    0.0\n",
            "2      2.0\n",
            "7      2.0\n",
            "33     2.0\n",
            "102    0.0\n",
            "270    0.0\n",
            "108    0.0\n",
            "118    3.0\n",
            "101    2.0\n",
            "27     3.0\n",
            "22     4.0\n",
            "76     4.0\n",
            "127    4.0\n",
            "296    0.0\n",
            "320    0.0\n",
            "397    0.0\n",
            "304    0.0\n",
            "385    0.0\n",
            "291    0.0\n",
            "1      4.0\n",
            "9      2.0\n",
            "257    0.0\n",
            "299    0.0\n",
            "20     2.0\n",
            "225    3.0\n",
            "285    0.0\n",
            "164    0.0\n",
            "Name: albumin, dtype: float64\n",
            "Int64Index([ 13,  17,  21,  30,  37,  50,  57,  59,  78,  81,  82,  86,  98,\n",
            "            104, 106, 109, 122, 125, 132, 134, 136, 142, 148, 151, 160, 162,\n",
            "            166, 174, 188, 197, 201, 202, 203, 205, 208, 222, 228, 231, 236,\n",
            "            238, 245, 268, 280, 295, 322, 346],\n",
            "           dtype='int64')\n",
            "220    0.0\n",
            "394    0.0\n",
            "331    0.0\n",
            "193    0.0\n",
            "138    0.0\n",
            "285    0.0\n",
            "80     0.0\n",
            "97     0.0\n",
            "2      3.0\n",
            "117    0.0\n",
            "347    0.0\n",
            "349    0.0\n",
            "217    0.0\n",
            "302    0.0\n",
            "77     0.0\n",
            "363    0.0\n",
            "360    0.0\n",
            "102    0.0\n",
            "388    0.0\n",
            "366    0.0\n",
            "264    0.0\n",
            "79     0.0\n",
            "173    0.0\n",
            "283    0.0\n",
            "116    0.0\n",
            "315    0.0\n",
            "88     0.0\n",
            "52     0.0\n",
            "310    0.0\n",
            "137    0.0\n",
            "89     0.0\n",
            "327    0.0\n",
            "384    0.0\n",
            "31     0.0\n",
            "176    0.0\n",
            "237    2.0\n",
            "181    0.0\n",
            "396    0.0\n",
            "291    0.0\n",
            "373    0.0\n",
            "213    1.0\n",
            "163    0.0\n",
            "75     0.0\n",
            "345    0.0\n",
            "383    0.0\n",
            "277    0.0\n",
            "209    0.0\n",
            "175    0.0\n",
            "72     3.0\n",
            "Name: sugar, dtype: float64\n",
            "Int64Index([ 13,  17,  21,  30,  37,  50,  57,  59,  78,  81,  82,  85,  86,\n",
            "             98, 104, 106, 109, 121, 122, 125, 132, 134, 136, 142, 148, 151,\n",
            "            160, 162, 166, 174, 188, 194, 197, 201, 202, 203, 205, 208, 222,\n",
            "            228, 231, 236, 238, 245, 268, 280, 295, 322, 346],\n",
            "           dtype='int64')\n",
            "224    117.0\n",
            "53     246.0\n",
            "120    323.0\n",
            "314    131.0\n",
            "367    125.0\n",
            "40      99.0\n",
            "17     114.0\n",
            "316     99.0\n",
            "91     210.0\n",
            "227    120.0\n",
            "399    131.0\n",
            "380    113.0\n",
            "43      79.0\n",
            "0      121.0\n",
            "277    123.0\n",
            "71     163.0\n",
            "341    130.0\n",
            "110    123.0\n",
            "369    107.0\n",
            "264    132.0\n",
            "70     360.0\n",
            "305    122.0\n",
            "374    111.0\n",
            "20     173.0\n",
            "203    207.0\n",
            "178     93.0\n",
            "182    131.0\n",
            "64     146.0\n",
            "49     144.0\n",
            "355     95.0\n",
            "15      76.0\n",
            "287    124.0\n",
            "81     360.0\n",
            "69     250.0\n",
            "289     93.0\n",
            "288     70.0\n",
            "381     79.0\n",
            "181    117.0\n",
            "313    104.0\n",
            "159    303.0\n",
            "136    213.0\n",
            "269    121.0\n",
            "27     264.0\n",
            "261    122.0\n",
            "Name: blood_glucose_random, dtype: float64\n",
            "Int64Index([  1,  21,  23,  24,  29,  38,  41,  47,  52,  54,  55,  56,  59,\n",
            "             65,  72,  75,  85, 112, 113, 114, 115, 122, 123, 131, 139, 141,\n",
            "            145, 161, 165, 186, 187, 193, 194, 197, 209, 215, 232, 234, 276,\n",
            "            283, 312, 315, 332, 378],\n",
            "           dtype='int64')\n",
            "58     142.0\n",
            "292     42.0\n",
            "84     186.0\n",
            "33      55.0\n",
            "119     27.0\n",
            "184    137.0\n",
            "365     46.0\n",
            "223     30.0\n",
            "300     26.0\n",
            "173      1.5\n",
            "343     35.0\n",
            "16      46.0\n",
            "389     48.0\n",
            "80      98.0\n",
            "236     66.0\n",
            "21     180.0\n",
            "231     51.0\n",
            "282     44.0\n",
            "74     107.0\n",
            "Name: blood_urea, dtype: float64\n",
            "Int64Index([ 23,  54,  55,  64,  67, 113, 134, 161, 165, 209, 215, 220, 232,\n",
            "            276, 283, 312, 315, 334, 378],\n",
            "           dtype='int64')\n",
            "9       7.2\n",
            "187     0.7\n",
            "263     1.2\n",
            "155     1.8\n",
            "121     1.3\n",
            "61     32.0\n",
            "19      1.6\n",
            "169     2.8\n",
            "143    18.0\n",
            "135     1.3\n",
            "279     0.6\n",
            "74      6.7\n",
            "357     1.1\n",
            "248     1.7\n",
            "175     2.2\n",
            "65      1.1\n",
            "343     0.8\n",
            "Name: serum_creatinine, dtype: float64\n",
            "Int64Index([23, 55, 64, 67, 113, 161, 165, 209, 215, 216, 220, 232, 276, 283,\n",
            "            312, 315, 334],\n",
            "           dtype='int64')\n",
            "63     131.0\n",
            "271    144.0\n",
            "262    135.0\n",
            "81     128.0\n",
            "275    139.0\n",
            "       ...  \n",
            "36     133.0\n",
            "174    141.0\n",
            "265    150.0\n",
            "281    147.0\n",
            "368    146.0\n",
            "Name: sodium, Length: 87, dtype: float64\n",
            "Int64Index([  0,   1,   2,   4,   7,   8,  10,  19,  23,  28,  29,  33,  34,\n",
            "             35,  40,  41,  47,  49,  52,  53,  55,  59,  64,  65,  67,  68,\n",
            "             69,  72,  79,  85,  86,  87,  88,  96, 103, 104, 109, 112, 113,\n",
            "            114, 115, 116, 118, 119, 123, 124, 131, 148, 149, 150, 155, 156,\n",
            "            161, 165, 167, 168, 169, 177, 179, 180, 187, 192, 194, 205, 207,\n",
            "            208, 209, 211, 214, 215, 216, 218, 219, 220, 222, 227, 228, 232,\n",
            "            234, 235, 237, 240, 283, 303, 315, 336, 363],\n",
            "           dtype='int64')\n",
            "93     2.9\n",
            "290    3.5\n",
            "319    4.4\n",
            "102    4.2\n",
            "246    5.7\n",
            "      ... \n",
            "320    4.7\n",
            "45     4.9\n",
            "139    4.1\n",
            "393    4.4\n",
            "257    3.7\n",
            "Name: potassium, Length: 88, dtype: float64\n",
            "Int64Index([  0,   1,   2,   4,   7,   8,  10,  19,  21,  23,  28,  29,  33,\n",
            "             34,  35,  40,  41,  47,  49,  52,  53,  55,  59,  64,  65,  67,\n",
            "             68,  69,  72,  79,  85,  86,  87,  88,  96, 103, 104, 109, 112,\n",
            "            113, 114, 115, 116, 118, 119, 123, 124, 131, 148, 149, 150, 155,\n",
            "            156, 161, 165, 167, 168, 169, 177, 179, 180, 187, 192, 194, 205,\n",
            "            207, 208, 209, 211, 214, 215, 216, 218, 219, 220, 222, 227, 228,\n",
            "            232, 234, 235, 237, 240, 283, 303, 315, 336, 363],\n",
            "           dtype='int64')\n",
            "308    13.9\n",
            "317    15.0\n",
            "1      11.3\n",
            "284    16.9\n",
            "354    14.6\n",
            "73      4.8\n",
            "139    11.1\n",
            "388    15.5\n",
            "31     10.1\n",
            "349    14.5\n",
            "163     9.0\n",
            "47     15.0\n",
            "124     9.1\n",
            "155    11.1\n",
            "189     9.5\n",
            "274    14.4\n",
            "275    16.5\n",
            "243    13.4\n",
            "151     9.6\n",
            "244    12.2\n",
            "370    16.2\n",
            "332    15.3\n",
            "18     12.7\n",
            "285    16.0\n",
            "196     8.1\n",
            "117    12.5\n",
            "384    15.4\n",
            "92     10.4\n",
            "355    15.0\n",
            "90     13.0\n",
            "111    10.0\n",
            "387    16.7\n",
            "206    13.8\n",
            "152    10.9\n",
            "261    17.0\n",
            "219     9.8\n",
            "45      9.3\n",
            "267    13.9\n",
            "326    15.8\n",
            "199     8.8\n",
            "20      7.7\n",
            "396    16.5\n",
            "379    16.1\n",
            "202     8.0\n",
            "214    13.7\n",
            "172    10.6\n",
            "75      8.1\n",
            "323    15.9\n",
            "160    10.9\n",
            "132     8.6\n",
            "72     10.3\n",
            "10      9.4\n",
            "Name: haemoglobin, dtype: float64\n",
            "Int64Index([ 23,  28,  30,  34,  41,  57,  60,  61,  66,  67,  82,  83,  86,\n",
            "             88,  89,  95, 100, 104, 113, 116, 119, 125, 138, 140, 142, 143,\n",
            "            146, 148, 156, 165, 166, 175, 183, 186, 192, 194, 211, 215, 221,\n",
            "            222, 224, 228, 230, 232, 233, 247, 273, 319, 324, 328, 330, 365],\n",
            "           dtype='int64')\n",
            "217    36.0\n",
            "101    33.0\n",
            "160    35.0\n",
            "5      39.0\n",
            "350    43.0\n",
            "       ... \n",
            "3      32.0\n",
            "6      36.0\n",
            "151    30.0\n",
            "398    51.0\n",
            "26     35.0\n",
            "Name: packed_cell_volume, Length: 71, dtype: float64\n",
            "Int64Index([ 13,  16,  17,  23,  28,  30,  34,  38,  41,  45,  57,  59,  60,\n",
            "             61,  64,  66,  67,  72,  75,  82,  83,  85,  86,  88,  89,  95,\n",
            "            100, 104, 109, 113, 114, 116, 118, 119, 120, 122, 125, 129, 136,\n",
            "            138, 140, 142, 143, 146, 148, 156, 165, 166, 175, 183, 186, 192,\n",
            "            194, 195, 197, 203, 209, 211, 215, 221, 222, 224, 228, 230, 232,\n",
            "            233, 247, 273, 319, 324, 365],\n",
            "           dtype='int64')\n",
            "58      7200.0\n",
            "223     9200.0\n",
            "14     11000.0\n",
            "375     9700.0\n",
            "340     6700.0\n",
            "        ...   \n",
            "381     5800.0\n",
            "199    10700.0\n",
            "53      8500.0\n",
            "393     7400.0\n",
            "132    13200.0\n",
            "Name: white_blood_cell_count, Length: 106, dtype: float64\n",
            "Int64Index([  6,  10,  13,  16,  17,  23,  28,  29,  30,  33,\n",
            "            ...\n",
            "            238, 239, 247, 273, 274, 287, 302, 319, 324, 330],\n",
            "           dtype='int64', length=106)\n",
            "161    4.8\n",
            "243    6.1\n",
            "70     5.2\n",
            "42     4.5\n",
            "359    4.5\n",
            "      ... \n",
            "225    4.5\n",
            "84     2.1\n",
            "244    4.6\n",
            "160    2.4\n",
            "153    2.9\n",
            "Name: red_blood_cell_count, Length: 131, dtype: float64\n",
            "Int64Index([  1,   2,   6,  10,  13,  16,  17,  23,  28,  29,\n",
            "            ...\n",
            "            238, 239, 247, 273, 274, 287, 302, 319, 324, 330],\n",
            "           dtype='int64', length=131)\n",
            "Series([], Name: class, dtype: int64)\n",
            "Int64Index([], dtype='int64')\n",
            "22       normal\n",
            "335      normal\n",
            "325      normal\n",
            "204      normal\n",
            "385      normal\n",
            "         ...   \n",
            "158      normal\n",
            "176      normal\n",
            "248    abnormal\n",
            "304      normal\n",
            "299      normal\n",
            "Name: red_blood_cells, Length: 152, dtype: object\n",
            "Int64Index([  0,   1,   5,   6,  10,  12,  13,  15,  16,  17,\n",
            "            ...\n",
            "            245, 268, 280, 290, 295, 309, 322, 349, 350, 381],\n",
            "           dtype='int64', length=152)\n",
            "246      normal\n",
            "189    abnormal\n",
            "372      normal\n",
            "298      normal\n",
            "127      normal\n",
            "         ...   \n",
            "173      normal\n",
            "330      normal\n",
            "136      normal\n",
            "323      normal\n",
            "239      normal\n",
            "Name: pus_cell, Length: 65, dtype: object\n",
            "Int64Index([  5,  13,  17,  21,  28,  30,  34,  37,  39,  43,  50,  53,  54,\n",
            "             59,  78,  81,  82,  85,  86,  98, 104, 106, 107, 109, 117, 121,\n",
            "            122, 125, 132, 134, 138, 142, 148, 151, 160, 161, 162, 164, 165,\n",
            "            166, 172, 174, 188, 197, 201, 202, 203, 205, 208, 221, 222, 228,\n",
            "            231, 236, 238, 245, 268, 280, 290, 295, 309, 322, 349, 350, 381],\n",
            "           dtype='int64')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[num_cols].isnull().sum()"
      ],
      "metadata": {
        "id": "hDZ3Uz7O5CdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d84e02-8bdf-4698-e73e-3e7479380e7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age                       0\n",
              "blood_pressure            0\n",
              "specific_gravity          0\n",
              "albumin                   0\n",
              "sugar                     0\n",
              "blood_glucose_random      0\n",
              "blood_urea                0\n",
              "serum_creatinine          0\n",
              "sodium                    0\n",
              "potassium                 0\n",
              "haemoglobin               0\n",
              "packed_cell_volume        0\n",
              "white_blood_cell_count    0\n",
              "red_blood_cell_count      0\n",
              "class                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_mode(feature):\n",
        "    mode = df[feature].mode()[0]\n",
        "    df[feature] = df[feature].fillna(mode)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for col in cat_cols:\n",
        "    \n",
        "    impute_mode(col)    "
      ],
      "metadata": {
        "id": "OLVvxKcP5RiR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[cat_cols].isnull().sum()"
      ],
      "metadata": {
        "id": "wuRVedNG5od1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ebd65b-81e5-45de-a0dc-f2301424820e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "red_blood_cells            0\n",
              "pus_cell                   0\n",
              "pus_cell_clumps            0\n",
              "bacteria                   0\n",
              "hypertension               0\n",
              "diabetes_mellitus          0\n",
              "coronary_artery_disease    0\n",
              "appetite                   0\n",
              "peda_edema                 0\n",
              "aanemia                    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for col in cat_cols:\n",
        "    print(f\"{col} has {df[col].nunique()} categories\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DqdV7Cou5ueL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d936d2b2-5986-4179-b019-41eb957879e3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red_blood_cells has 2 categories\n",
            "\n",
            "pus_cell has 2 categories\n",
            "\n",
            "pus_cell_clumps has 2 categories\n",
            "\n",
            "bacteria has 2 categories\n",
            "\n",
            "hypertension has 2 categories\n",
            "\n",
            "diabetes_mellitus has 2 categories\n",
            "\n",
            "coronary_artery_disease has 2 categories\n",
            "\n",
            "appetite has 2 categories\n",
            "\n",
            "peda_edema has 2 categories\n",
            "\n",
            "aanemia has 2 categories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "print(le)\n",
        "for col in cat_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    print(df[col])\n",
        "\n"
      ],
      "metadata": {
        "id": "gLpTWVEE51cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f8fd3a-659e-4235-e677-7c51a7acde4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LabelEncoder()\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n",
            "      ..\n",
            "395    1\n",
            "396    1\n",
            "397    1\n",
            "398    1\n",
            "399    1\n",
            "Name: red_blood_cells, Length: 400, dtype: int64\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      0\n",
            "4      1\n",
            "      ..\n",
            "395    1\n",
            "396    1\n",
            "397    1\n",
            "398    1\n",
            "399    1\n",
            "Name: pus_cell, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: pus_cell_clumps, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: bacteria, Length: 400, dtype: int64\n",
            "0      1\n",
            "1      0\n",
            "2      0\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: hypertension, Length: 400, dtype: int64\n",
            "0      1\n",
            "1      0\n",
            "2      1\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: diabetes_mellitus, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      0\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: coronary_artery_disease, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      1\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: appetite, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      0\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: peda_edema, Length: 400, dtype: int64\n",
            "0      0\n",
            "1      0\n",
            "2      1\n",
            "3      1\n",
            "4      0\n",
            "      ..\n",
            "395    0\n",
            "396    0\n",
            "397    0\n",
            "398    0\n",
            "399    0\n",
            "Name: aanemia, Length: 400, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "HtP1Dkfw6AG7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "9f4b91ee-fa04-414c-e3bb-86d993887e4c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    age  blood_pressure  specific_gravity  albumin  sugar  red_blood_cells  \\\n",
              "0  48.0            80.0             1.020      1.0    0.0                1   \n",
              "1   7.0            50.0             1.020      4.0    0.0                1   \n",
              "2  62.0            80.0             1.010      2.0    3.0                1   \n",
              "3  48.0            70.0             1.005      4.0    0.0                1   \n",
              "4  51.0            80.0             1.010      2.0    0.0                1   \n",
              "\n",
              "   pus_cell  pus_cell_clumps  bacteria  blood_glucose_random  ...  \\\n",
              "0         1                0         0                 121.0  ...   \n",
              "1         1                0         0                 117.0  ...   \n",
              "2         1                0         0                 423.0  ...   \n",
              "3         0                1         0                 117.0  ...   \n",
              "4         1                0         0                 106.0  ...   \n",
              "\n",
              "   packed_cell_volume  white_blood_cell_count  red_blood_cell_count  \\\n",
              "0                44.0                  7800.0                   5.2   \n",
              "1                38.0                  6000.0                   4.8   \n",
              "2                31.0                  7500.0                   6.1   \n",
              "3                32.0                  6700.0                   3.9   \n",
              "4                35.0                  7300.0                   4.6   \n",
              "\n",
              "   hypertension  diabetes_mellitus  coronary_artery_disease  appetite  \\\n",
              "0             1                  1                        0         0   \n",
              "1             0                  0                        0         0   \n",
              "2             0                  1                        0         1   \n",
              "3             1                  0                        0         1   \n",
              "4             0                  0                        0         0   \n",
              "\n",
              "   peda_edema  aanemia  class  \n",
              "0           0        0      1  \n",
              "1           0        0      1  \n",
              "2           0        1      1  \n",
              "3           1        1      1  \n",
              "4           0        0      1  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae1bdb7e-f959-4676-b32b-bf12db658c53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>blood_pressure</th>\n",
              "      <th>specific_gravity</th>\n",
              "      <th>albumin</th>\n",
              "      <th>sugar</th>\n",
              "      <th>red_blood_cells</th>\n",
              "      <th>pus_cell</th>\n",
              "      <th>pus_cell_clumps</th>\n",
              "      <th>bacteria</th>\n",
              "      <th>blood_glucose_random</th>\n",
              "      <th>...</th>\n",
              "      <th>packed_cell_volume</th>\n",
              "      <th>white_blood_cell_count</th>\n",
              "      <th>red_blood_cell_count</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>diabetes_mellitus</th>\n",
              "      <th>coronary_artery_disease</th>\n",
              "      <th>appetite</th>\n",
              "      <th>peda_edema</th>\n",
              "      <th>aanemia</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44.0</td>\n",
              "      <td>7800.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>6000.0</td>\n",
              "      <td>4.8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>423.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>32.0</td>\n",
              "      <td>6700.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>106.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.0</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>4.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1bdb7e-f959-4676-b32b-bf12db658c53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae1bdb7e-f959-4676-b32b-bf12db658c53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae1bdb7e-f959-4676-b32b-bf12db658c53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_col = [col for col in df.columns if col != 'class']\n",
        "dep_col = 'class'\n",
        "\n",
        "X = df[ind_col]\n",
        "Y = df[dep_col]"
      ],
      "metadata": {
        "id": "cw_cYruQ6PSJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score,KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "num_folds=5\n",
        "logreg=LogisticRegression()\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "score=cross_val_score(logreg,X,Y,cv=kfold)\n",
        "print(\"Cross Validation Scores are {}\".format(score))\n",
        "print(\"Average Cross Validation score :{}\".format(score.mean()))"
      ],
      "metadata": {
        "id": "EAXJHVBM6Zu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fbbd46-e540-4e55-881d-85069c7b6f7c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation Scores are [0.9125 0.9    0.875  0.8875 0.8875]\n",
            "Average Cross Validation score :0.8925000000000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=24))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['acc'])\n",
        "\n",
        "model.summary()  "
      ],
      "metadata": {
        "id": "jnv36859-c9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f523d3e-8bff-4738-aca5-17aa0f1b0d0e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_136\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_298 (Dense)           (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50\n",
            "Trainable params: 50\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "\n",
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "for train, test in kfold.split(X , Y): \n",
        "  X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "  Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_test = to_categorical(Y_test)\n",
        "  #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(X_train, Y_train ,\n",
        "              validation_data=(X_test , Y_test) ,       \n",
        "              batch_size=10,\n",
        "              epochs=5\n",
        "              )\n",
        "  acc= history.history['acc']\n",
        "  loss=history.history['loss']\n",
        "  val_acc=history.history['val_acc']\n",
        "  val_loss=history.history['val_loss']\n",
        "  # Generate generalization metrics\n",
        "  #scores2 = model.evaluate(X_train,Y_train )\n",
        "  scores = model.evaluate(X_test,Y_test )\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  #acc_test_per_fold.append(scores[1] * 100)\n",
        "  #loss_test_per_fold.append(scores[0])\n",
        "\n",
        "  #acc2_per_fold.append(scores2[1] * 100)\n",
        "  #loss2_per_fold.append(scores2[0])\n",
        "  loss_test_per_fold.append(val_loss)\n",
        "  acc_test_per_fold.append(val_acc)\n",
        "  acc_per_fold.append(acc)\n",
        "  loss_per_fold.append(loss)\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "plt.plot(acc_test, 'r^-', label='test accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "acc_per_fold= np.array(acc_per_fold)\n",
        "acc=np.sum(acc_per_fold , axis=0)/5\n",
        "plt.plot(acc, 'bo-', label='Train acc')\n",
        "plt.grid()\n",
        "plt.legend()  "
      ],
      "metadata": {
        "id": "uyAZpEjrEglB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c531ceb-fa2c-4ace-e145-5b7c48071e55"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 6.9956 - acc: 0.4281 - val_loss: 6.7592 - val_acc: 0.4000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 6.1709 - acc: 0.4406 - val_loss: 6.0469 - val_acc: 0.4000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.3406 - acc: 0.4406 - val_loss: 5.2456 - val_acc: 0.4250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.7123 - acc: 0.4437 - val_loss: 5.0295 - val_acc: 0.4375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.1706 - acc: 0.4750 - val_loss: 3.9902 - val_acc: 0.5250\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 3.9902 - acc: 0.5250\n",
            "Score for fold 1: loss of 3.9901747703552246; acc of 52.49999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3651 - acc: 0.5562 - val_loss: 2.5296 - val_acc: 0.6125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8636 - acc: 0.5656 - val_loss: 1.9636 - val_acc: 0.5500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 2.2659 - acc: 0.5281 - val_loss: 1.6945 - val_acc: 0.5625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.8335 - acc: 0.5344 - val_loss: 1.3691 - val_acc: 0.5375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.5491 - acc: 0.5063 - val_loss: 1.1593 - val_acc: 0.5000\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1593 - acc: 0.5000\n",
            "Score for fold 2: loss of 1.1593360900878906; acc of 50.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.0807 - acc: 0.4688 - val_loss: 1.6775 - val_acc: 0.5250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.0116 - acc: 0.4844 - val_loss: 1.4139 - val_acc: 0.5375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.9694 - acc: 0.5094 - val_loss: 1.3089 - val_acc: 0.5625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.9500 - acc: 0.5219 - val_loss: 1.1709 - val_acc: 0.5625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8983 - acc: 0.5219 - val_loss: 0.9526 - val_acc: 0.5625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9526 - acc: 0.5625\n",
            "Score for fold 3: loss of 0.95257568359375; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.8825 - acc: 0.5469 - val_loss: 0.7178 - val_acc: 0.5500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.8623 - acc: 0.5594 - val_loss: 0.7024 - val_acc: 0.5500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8483 - acc: 0.5594 - val_loss: 0.6895 - val_acc: 0.5500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.8360 - acc: 0.5688 - val_loss: 0.6773 - val_acc: 0.5625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.8245 - acc: 0.5781 - val_loss: 0.6652 - val_acc: 0.5625\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6652 - acc: 0.5625\n",
            "Score for fold 4: loss of 0.665174126625061; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7677 - acc: 0.5938 - val_loss: 0.8368 - val_acc: 0.5000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7567 - acc: 0.5969 - val_loss: 0.8272 - val_acc: 0.5125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7460 - acc: 0.6125 - val_loss: 0.8170 - val_acc: 0.5125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7360 - acc: 0.6125 - val_loss: 0.8071 - val_acc: 0.5250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7110 - acc: 0.6219 - val_loss: 0.7987 - val_acc: 0.5250\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7987 - acc: 0.5250\n",
            "Score for fold 5: loss of 0.7986501455307007; acc of 52.49999761581421%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efbe38a5f70>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(40, input_dim=len(X.columns)))\n",
        "model2.add(Dense(2 ))\n",
        "\n",
        "\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['acc'])\n",
        "model2.summary()  \n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "for train, test in kfold.split(X , Y): \n",
        "  X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "  Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_test = to_categorical(Y_test)\n",
        "  #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model2.fit(X_train, Y_train ,\n",
        "              validation_data=(X_test , Y_test) ,       \n",
        "              batch_size=10,\n",
        "              epochs=5\n",
        "              )\n",
        "  acc= history.history['acc']\n",
        "  loss=history.history['loss']\n",
        "  val_acc=history.history['val_acc']\n",
        "  val_loss=history.history['val_loss']\n",
        "\n",
        "  loss_test_per_fold.append(val_loss)\n",
        "  acc_test_per_fold.append(val_acc)\n",
        "  acc_per_fold.append(acc)\n",
        "  loss_per_fold.append(loss)\n",
        "\n",
        "\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "plt.plot(acc_test, 'r^-', label='test accuracy')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "acc_per_fold= np.array(acc_per_fold)\n",
        "acc=np.sum(acc_per_fold , axis=0)/5\n",
        "plt.plot(acc, 'bo-', label='Train acc')\n",
        "plt.grid()\n",
        "plt.legend()  \n"
      ],
      "metadata": {
        "id": "YwQ2O2Zgv-k2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d89e1c93-82a1-4b43-e032-977041b92e77"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_139\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_303 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_304 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 3.0016 - acc: 0.5688 - val_loss: 0.7725 - val_acc: 0.7375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7452 - acc: 0.7781 - val_loss: 0.4640 - val_acc: 0.8000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5547 - acc: 0.8531 - val_loss: 0.4117 - val_acc: 0.8375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4840 - acc: 0.8938 - val_loss: 0.3842 - val_acc: 0.8375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4124 - acc: 0.9156 - val_loss: 0.3699 - val_acc: 0.8500\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3977 - acc: 0.9000 - val_loss: 0.3258 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3822 - acc: 0.9031 - val_loss: 0.3133 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3701 - acc: 0.9000 - val_loss: 0.3180 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3579 - acc: 0.9094 - val_loss: 0.2927 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3480 - acc: 0.9094 - val_loss: 0.2882 - val_acc: 0.9375\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2925 - acc: 0.9187 - val_loss: 0.4699 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2836 - acc: 0.9312 - val_loss: 0.4643 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2741 - acc: 0.9312 - val_loss: 0.4528 - val_acc: 0.9125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2668 - acc: 0.9250 - val_loss: 0.4467 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2598 - acc: 0.9250 - val_loss: 0.4424 - val_acc: 0.9125\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2981 - acc: 0.9250 - val_loss: 0.2576 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2936 - acc: 0.9250 - val_loss: 0.2527 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2866 - acc: 0.9250 - val_loss: 0.2478 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2822 - acc: 0.9219 - val_loss: 0.2431 - val_acc: 0.9250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2776 - acc: 0.9219 - val_loss: 0.2393 - val_acc: 0.9250\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2826 - acc: 0.9125 - val_loss: 0.2026 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2787 - acc: 0.9187 - val_loss: 0.2007 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2748 - acc: 0.9125 - val_loss: 0.1949 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2710 - acc: 0.9156 - val_loss: 0.1957 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2672 - acc: 0.9156 - val_loss: 0.1910 - val_acc: 0.9625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efbe4fa4370>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "g=0\n",
        "figure, axis = plt.subplots(3, 3)\n",
        "\n",
        "\n",
        " \n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "for i in [2 , 4 , 8 , 16 , 32 , 64,  128 , 256 , 512] :\n",
        "  loss_test_per_fold=[]\n",
        "  acc_test_per_fold=[]\n",
        "  acc_per_fold=[]\n",
        "  loss_per_fold=[]\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(i, input_dim=24))\n",
        "  model1.add(Dense(2))\n",
        "\n",
        "  model1.compile(optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc'])\n",
        "  model1.summary() \n",
        "  for train, test in kfold.split(X , Y): \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    Y_test = to_categorical(Y_test)\n",
        "    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    \n",
        "     \n",
        "    \n",
        "    # Fit data to model\n",
        "    history = model1.fit(X_train, Y_train ,\n",
        "                validation_data=(X_test , Y_test) ,       \n",
        "                batch_size=10,\n",
        "                epochs=5\n",
        "                )\n",
        "    acc= history.history['acc']\n",
        "    loss=history.history['loss']\n",
        "    val_acc=history.history['val_acc']\n",
        "    val_loss=history.history['val_loss']\n",
        "    scores = model.evaluate(X_test,Y_test )\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    loss_test_per_fold.append(val_loss)\n",
        "    acc_test_per_fold.append(val_acc)\n",
        "    acc_per_fold.append(acc)\n",
        "    loss_per_fold.append(loss)\n",
        "\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "  \n",
        "  acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "  acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "  axis[g//3 , g%3].plot(acc_test, 'r^-', label='test accuracy')\n",
        "  axis[g//3 , g%3].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "\n",
        "  acc_per_fold= np.array(acc_per_fold)\n",
        "  acc=np.sum(acc_per_fold , axis=0)/5\n",
        "  axis[g//3 , g%3].plot(acc, 'bo-', label='Train acc')\n",
        "  axis[g//3 , g%3].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()  \n",
        "\n",
        "  g=g+1"
      ],
      "metadata": {
        "id": "jwspiPBhySKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9b582ae-93f5-461f-d652-4bc55255d1ef"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_140\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_305 (Dense)           (None, 2)                 50        \n",
            "                                                                 \n",
            " dense_306 (Dense)           (None, 2)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56\n",
            "Trainable params: 56\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 2.3232 - acc: 0.5344 - val_loss: 1.5974 - val_acc: 0.5000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.4077 - acc: 0.5656 - val_loss: 1.0531 - val_acc: 0.5875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.1236 - acc: 0.6031 - val_loss: 0.9365 - val_acc: 0.6000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.9595 - acc: 0.6125 - val_loss: 0.9022 - val_acc: 0.6375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8678 - acc: 0.6187 - val_loss: 0.8890 - val_acc: 0.6375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7389 - acc: 0.6125\n",
            "Score for fold 1: loss of 0.7388877868652344; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.8781 - acc: 0.6062 - val_loss: 0.7629 - val_acc: 0.6875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8668 - acc: 0.6062 - val_loss: 0.7562 - val_acc: 0.6875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.8578 - acc: 0.6062 - val_loss: 0.7510 - val_acc: 0.6875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8503 - acc: 0.6062 - val_loss: 0.7466 - val_acc: 0.6875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7914 - acc: 0.6062 - val_loss: 0.7340 - val_acc: 0.6875\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6659 - acc: 0.6375\n",
            "Score for fold 2: loss of 0.6659413576126099; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7558 - acc: 0.6375 - val_loss: 0.7248 - val_acc: 0.5625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7472 - acc: 0.6375 - val_loss: 0.7170 - val_acc: 0.5625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7400 - acc: 0.6375 - val_loss: 0.7108 - val_acc: 0.5625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7335 - acc: 0.6344 - val_loss: 0.7044 - val_acc: 0.5625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7271 - acc: 0.6313 - val_loss: 0.6974 - val_acc: 0.5625\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6658 - acc: 0.5875\n",
            "Score for fold 3: loss of 0.665765643119812; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.7181 - acc: 0.6313 - val_loss: 0.7004 - val_acc: 0.5750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7119 - acc: 0.6281 - val_loss: 0.6917 - val_acc: 0.5750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7050 - acc: 0.6281 - val_loss: 0.6832 - val_acc: 0.5750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6900 - acc: 0.6219 - val_loss: 0.7567 - val_acc: 0.5625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.7210 - acc: 0.6156 - val_loss: 0.7218 - val_acc: 0.5750\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7025 - acc: 0.5375\n",
            "Score for fold 4: loss of 0.7024722099304199; acc of 53.75000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6972 - acc: 0.6094 - val_loss: 0.6335 - val_acc: 0.6375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6782 - acc: 0.6125 - val_loss: 0.6182 - val_acc: 0.6375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6680 - acc: 0.6125 - val_loss: 0.6093 - val_acc: 0.6375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6615 - acc: 0.6125 - val_loss: 0.6020 - val_acc: 0.6375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6558 - acc: 0.6125 - val_loss: 0.5962 - val_acc: 0.6375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8362 - acc: 0.6375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 0.8362027406692505; acc of 63.749998807907104%\n",
            "Model: \"sequential_141\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_307 (Dense)           (None, 4)                 100       \n",
            "                                                                 \n",
            " dense_308 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110\n",
            "Trainable params: 110\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 5.0083 - acc: 0.3844 - val_loss: 5.5333 - val_acc: 0.3500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.4990 - acc: 0.4656 - val_loss: 5.1028 - val_acc: 0.4000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.2030 - acc: 0.4938 - val_loss: 4.8080 - val_acc: 0.4125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.0458 - acc: 0.5188 - val_loss: 4.7583 - val_acc: 0.4500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.9696 - acc: 0.5344 - val_loss: 4.6225 - val_acc: 0.4750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7300 - acc: 0.6125\n",
            "Score for fold 6: loss of 0.7300351858139038; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.1289 - acc: 0.5281 - val_loss: 3.7037 - val_acc: 0.5875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.0517 - acc: 0.5500 - val_loss: 3.7242 - val_acc: 0.6000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.9378 - acc: 0.5625 - val_loss: 3.4559 - val_acc: 0.6000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.8645 - acc: 0.5813 - val_loss: 3.4412 - val_acc: 0.6250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.8361 - acc: 0.5906 - val_loss: 3.5254 - val_acc: 0.6250\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7087 - acc: 0.6750\n",
            "Score for fold 7: loss of 0.7086933851242065; acc of 67.5000011920929%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.7919 - acc: 0.5938 - val_loss: 3.3614 - val_acc: 0.6500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.5716 - acc: 0.6031 - val_loss: 3.2098 - val_acc: 0.6625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4982 - acc: 0.6156 - val_loss: 3.1784 - val_acc: 0.6625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.4676 - acc: 0.6156 - val_loss: 3.1521 - val_acc: 0.6625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.4269 - acc: 0.6156 - val_loss: 3.0744 - val_acc: 0.6625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7688 - acc: 0.5875\n",
            "Score for fold 8: loss of 0.7687805891036987; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3378 - acc: 0.6219 - val_loss: 3.3548 - val_acc: 0.6375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3167 - acc: 0.6219 - val_loss: 3.3456 - val_acc: 0.6375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.3065 - acc: 0.6219 - val_loss: 3.3389 - val_acc: 0.6375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2718 - acc: 0.6219 - val_loss: 3.1829 - val_acc: 0.6375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 3.2109 - acc: 0.6219 - val_loss: 3.1719 - val_acc: 0.6375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7517 - acc: 0.5875\n",
            "Score for fold 9: loss of 0.7516579627990723; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1398 - acc: 0.6344 - val_loss: 3.3941 - val_acc: 0.5875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1291 - acc: 0.6344 - val_loss: 3.3830 - val_acc: 0.5875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1216 - acc: 0.6344 - val_loss: 3.3773 - val_acc: 0.5875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1156 - acc: 0.6344 - val_loss: 3.3690 - val_acc: 0.5875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1106 - acc: 0.6344 - val_loss: 3.3660 - val_acc: 0.5875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6501 - acc: 0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 10: loss of 0.6501026153564453; acc of 55.000001192092896%\n",
            "Model: \"sequential_142\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_309 (Dense)           (None, 8)                 200       \n",
            "                                                                 \n",
            " dense_310 (Dense)           (None, 2)                 18        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 218\n",
            "Trainable params: 218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 11 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 5.6111 - acc: 0.3812 - val_loss: 5.3697 - val_acc: 0.3500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.1997 - acc: 0.3812 - val_loss: 5.1898 - val_acc: 0.3500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 5.0204 - acc: 0.3812 - val_loss: 5.1617 - val_acc: 0.3500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9548 - acc: 0.3812 - val_loss: 5.1437 - val_acc: 0.3500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9245 - acc: 0.3812 - val_loss: 5.1353 - val_acc: 0.3500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7693 - acc: 0.5875\n",
            "Score for fold 11: loss of 0.769271731376648; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 12 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.9739 - acc: 0.3719 - val_loss: 4.8526 - val_acc: 0.3875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9617 - acc: 0.3719 - val_loss: 4.8444 - val_acc: 0.3875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9534 - acc: 0.3719 - val_loss: 4.8351 - val_acc: 0.3875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9463 - acc: 0.3719 - val_loss: 4.8279 - val_acc: 0.3875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9400 - acc: 0.3719 - val_loss: 4.8205 - val_acc: 0.3875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7663 - acc: 0.6000\n",
            "Score for fold 12: loss of 0.7662509679794312; acc of 60.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 13 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.8969 - acc: 0.3812 - val_loss: 4.9666 - val_acc: 0.3625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8916 - acc: 0.3812 - val_loss: 4.9628 - val_acc: 0.3625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8864 - acc: 0.3812 - val_loss: 4.9602 - val_acc: 0.3625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8830 - acc: 0.3812 - val_loss: 4.9576 - val_acc: 0.3625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8791 - acc: 0.3812 - val_loss: 4.9553 - val_acc: 0.3625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7086 - acc: 0.6500\n",
            "Score for fold 13: loss of 0.7085906863212585; acc of 64.99999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 14 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.8501 - acc: 0.3812 - val_loss: 5.0553 - val_acc: 0.3625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8474 - acc: 0.3812 - val_loss: 5.0520 - val_acc: 0.3625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8452 - acc: 0.3812 - val_loss: 5.0506 - val_acc: 0.3625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8429 - acc: 0.3812 - val_loss: 5.0481 - val_acc: 0.3625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.8418 - acc: 0.3812 - val_loss: 5.0495 - val_acc: 0.3625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8207 - acc: 0.6125\n",
            "Score for fold 14: loss of 0.8206733465194702; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 15 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.9644 - acc: 0.3688 - val_loss: 4.5478 - val_acc: 0.4125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9633 - acc: 0.3688 - val_loss: 4.5481 - val_acc: 0.4125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 4.9606 - acc: 0.3688 - val_loss: 4.5459 - val_acc: 0.4125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9589 - acc: 0.3688 - val_loss: 4.5453 - val_acc: 0.4125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 4.9578 - acc: 0.3688 - val_loss: 4.5450 - val_acc: 0.4125\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5445 - acc: 0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 15: loss of 0.5444830656051636; acc of 56.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_143\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_311 (Dense)           (None, 16)                400       \n",
            "                                                                 \n",
            " dense_312 (Dense)           (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 434\n",
            "Trainable params: 434\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 16 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 1.7752 - acc: 0.5625 - val_loss: 0.9925 - val_acc: 0.4750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 1.0271 - acc: 0.5031 - val_loss: 0.7491 - val_acc: 0.5250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8380 - acc: 0.5469 - val_loss: 0.7394 - val_acc: 0.5750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6373 - acc: 0.5969 - val_loss: 0.5454 - val_acc: 0.5500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5396 - acc: 0.6781 - val_loss: 0.4759 - val_acc: 0.8375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7748 - acc: 0.6125\n",
            "Score for fold 16: loss of 0.7747606039047241; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 17 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.4810 - acc: 0.8594 - val_loss: 0.4706 - val_acc: 0.9000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4425 - acc: 0.8781 - val_loss: 0.4323 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4101 - acc: 0.8844 - val_loss: 0.4047 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3801 - acc: 0.8813 - val_loss: 0.3882 - val_acc: 0.9250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3550 - acc: 0.8906 - val_loss: 0.4339 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7589 - acc: 0.6375\n",
            "Score for fold 17: loss of 0.7589136362075806; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 18 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3529 - acc: 0.8938 - val_loss: 0.2675 - val_acc: 0.9375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2969 - acc: 0.9062 - val_loss: 0.2525 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2548 - acc: 0.9062 - val_loss: 0.2330 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2351 - acc: 0.9125 - val_loss: 0.2082 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.2178 - acc: 0.9187 - val_loss: 0.1899 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7743 - acc: 0.5625\n",
            "Score for fold 18: loss of 0.7742551565170288; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 19 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1811 - acc: 0.9281 - val_loss: 0.2486 - val_acc: 0.8875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1665 - acc: 0.9250 - val_loss: 0.2332 - val_acc: 0.8875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.1524 - acc: 0.9250 - val_loss: 0.2184 - val_acc: 0.8875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1415 - acc: 0.9312 - val_loss: 0.2039 - val_acc: 0.8875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1340 - acc: 0.9375 - val_loss: 0.1978 - val_acc: 0.8875\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6053 - acc: 0.5750\n",
            "Score for fold 19: loss of 0.6052719950675964; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 20 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1378 - acc: 0.9312 - val_loss: 0.1267 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1292 - acc: 0.9312 - val_loss: 0.1223 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1211 - acc: 0.9375 - val_loss: 0.1174 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.1151 - acc: 0.9469 - val_loss: 0.1155 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1088 - acc: 0.9531 - val_loss: 0.1117 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6961 - acc: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 20: loss of 0.6960684061050415; acc of 62.5%\n",
            "Model: \"sequential_144\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_313 (Dense)           (None, 32)                800       \n",
            "                                                                 \n",
            " dense_314 (Dense)           (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 866\n",
            "Trainable params: 866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 21 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 1.2263 - acc: 0.6281 - val_loss: 0.7310 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7889 - acc: 0.8500 - val_loss: 0.5115 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6447 - acc: 0.8719 - val_loss: 0.4363 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5448 - acc: 0.8781 - val_loss: 0.3924 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3684 - acc: 0.8938 - val_loss: 0.2831 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6312 - acc: 0.5875\n",
            "Score for fold 21: loss of 0.63116854429245; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 22 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2679 - acc: 0.9281 - val_loss: 0.2445 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2217 - acc: 0.9312 - val_loss: 0.1948 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2029 - acc: 0.9219 - val_loss: 0.1856 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1874 - acc: 0.9219 - val_loss: 0.1634 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1722 - acc: 0.9219 - val_loss: 0.1509 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6666 - acc: 0.5750\n",
            "Score for fold 22: loss of 0.6665742993354797; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 23 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1573 - acc: 0.9250 - val_loss: 0.1503 - val_acc: 0.9375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1450 - acc: 0.9219 - val_loss: 0.1368 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1358 - acc: 0.9312 - val_loss: 0.1274 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1267 - acc: 0.9531 - val_loss: 0.1215 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1175 - acc: 0.9438 - val_loss: 0.1143 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9299 - acc: 0.6250\n",
            "Score for fold 23: loss of 0.9298813939094543; acc of 62.5%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 24 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1039 - acc: 0.9531 - val_loss: 0.1499 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0973 - acc: 0.9563 - val_loss: 0.1390 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0953 - acc: 0.9563 - val_loss: 0.2566 - val_acc: 0.9000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0910 - acc: 0.9469 - val_loss: 0.1313 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0837 - acc: 0.9594 - val_loss: 0.1323 - val_acc: 0.9250\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7340 - acc: 0.6375\n",
            "Score for fold 24: loss of 0.7339507341384888; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 25 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0853 - acc: 0.9563 - val_loss: 0.1236 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0780 - acc: 0.9563 - val_loss: 0.1618 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0750 - acc: 0.9594 - val_loss: 0.1316 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0712 - acc: 0.9625 - val_loss: 0.1239 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0700 - acc: 0.9625 - val_loss: 0.1415 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6477 - acc: 0.5875\n",
            "Score for fold 25: loss of 0.6476948857307434; acc of 58.74999761581421%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_145\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_315 (Dense)           (None, 64)                1600      \n",
            "                                                                 \n",
            " dense_316 (Dense)           (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,730\n",
            "Trainable params: 1,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 26 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 2.7574 - acc: 0.6000 - val_loss: 0.4024 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3840 - acc: 0.9219 - val_loss: 0.3320 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2961 - acc: 0.9312 - val_loss: 0.2798 - val_acc: 0.9125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2574 - acc: 0.9156 - val_loss: 0.2503 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2298 - acc: 0.9156 - val_loss: 0.2260 - val_acc: 0.9125\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7325 - acc: 0.6000\n",
            "Score for fold 26: loss of 0.7324510216712952; acc of 60.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 27 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2101 - acc: 0.9187 - val_loss: 0.2077 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1903 - acc: 0.9156 - val_loss: 0.1865 - val_acc: 0.9000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1718 - acc: 0.9219 - val_loss: 0.1763 - val_acc: 0.9125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1603 - acc: 0.9312 - val_loss: 0.1658 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1448 - acc: 0.9344 - val_loss: 0.1500 - val_acc: 0.9125\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6320 - acc: 0.6375\n",
            "Score for fold 27: loss of 0.632032036781311; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 28 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1308 - acc: 0.9312 - val_loss: 0.1599 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1212 - acc: 0.9406 - val_loss: 0.1501 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1130 - acc: 0.9406 - val_loss: 0.1438 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1069 - acc: 0.9469 - val_loss: 0.1396 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0997 - acc: 0.9500 - val_loss: 0.1358 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7428 - acc: 0.5500\n",
            "Score for fold 28: loss of 0.7427524328231812; acc of 55.000001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 29 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1076 - acc: 0.9500 - val_loss: 0.0866 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1032 - acc: 0.9469 - val_loss: 0.0843 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1013 - acc: 0.9500 - val_loss: 0.0873 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0981 - acc: 0.9563 - val_loss: 0.0835 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0952 - acc: 0.9625 - val_loss: 0.0831 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7581 - acc: 0.5875\n",
            "Score for fold 29: loss of 0.7581104040145874; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 30 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0962 - acc: 0.9531 - val_loss: 0.0617 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0928 - acc: 0.9563 - val_loss: 0.0614 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0930 - acc: 0.9563 - val_loss: 0.0624 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0915 - acc: 0.9563 - val_loss: 0.0608 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0885 - acc: 0.9563 - val_loss: 0.0569 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7439 - acc: 0.6375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 30: loss of 0.7439238429069519; acc of 63.749998807907104%\n",
            "Model: \"sequential_146\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_317 (Dense)           (None, 128)               3200      \n",
            "                                                                 \n",
            " dense_318 (Dense)           (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,458\n",
            "Trainable params: 3,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 31 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 0.5423 - acc: 0.8031 - val_loss: 0.3419 - val_acc: 0.8875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2336 - acc: 0.9406 - val_loss: 0.2310 - val_acc: 0.8875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1603 - acc: 0.9406 - val_loss: 0.1849 - val_acc: 0.9000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1267 - acc: 0.9469 - val_loss: 0.1615 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1106 - acc: 0.9500 - val_loss: 0.1450 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7801 - acc: 0.6375\n",
            "Score for fold 31: loss of 0.7800723910331726; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 32 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1611 - acc: 0.9375 - val_loss: 0.0533 - val_acc: 1.0000\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1338 - acc: 0.9438 - val_loss: 0.0460 - val_acc: 1.0000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1276 - acc: 0.9531 - val_loss: 0.0390 - val_acc: 1.0000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1492 - acc: 0.9406 - val_loss: 0.0520 - val_acc: 1.0000\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1141 - acc: 0.9594 - val_loss: 0.0430 - val_acc: 1.0000\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5713 - acc: 0.5875\n",
            "Score for fold 32: loss of 0.5712896585464478; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 33 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0991 - acc: 0.9719 - val_loss: 0.0764 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0928 - acc: 0.9719 - val_loss: 0.0867 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0907 - acc: 0.9719 - val_loss: 0.0715 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0855 - acc: 0.9719 - val_loss: 0.0662 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0863 - acc: 0.9750 - val_loss: 0.0661 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6593 - acc: 0.6375\n",
            "Score for fold 33: loss of 0.6593395471572876; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 34 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1044 - acc: 0.9812 - val_loss: 0.0750 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0755 - acc: 0.9812 - val_loss: 0.0894 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0972 - acc: 0.9812 - val_loss: 0.0747 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0896 - acc: 0.9875 - val_loss: 0.0697 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0876 - acc: 0.9906 - val_loss: 0.0716 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6847 - acc: 0.5750\n",
            "Score for fold 34: loss of 0.6846798658370972; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 35 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0483 - acc: 0.9812 - val_loss: 0.2228 - val_acc: 0.9875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0457 - acc: 0.9781 - val_loss: 0.2222 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0427 - acc: 0.9781 - val_loss: 0.2217 - val_acc: 0.9875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0407 - acc: 0.9812 - val_loss: 0.2254 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0395 - acc: 0.9812 - val_loss: 0.2188 - val_acc: 0.9875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9139 - acc: 0.5750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 35: loss of 0.9138882756233215; acc of 57.499998807907104%\n",
            "Model: \"sequential_147\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_319 (Dense)           (None, 256)               6400      \n",
            "                                                                 \n",
            " dense_320 (Dense)           (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,914\n",
            "Trainable params: 6,914\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 36 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 3.0647 - acc: 0.6281 - val_loss: 3.0857 - val_acc: 0.6125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9316 - acc: 0.6281 - val_loss: 3.0669 - val_acc: 0.6125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9188 - acc: 0.6281 - val_loss: 3.0544 - val_acc: 0.6125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9128 - acc: 0.6281 - val_loss: 3.0597 - val_acc: 0.6125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9111 - acc: 0.6281 - val_loss: 3.1221 - val_acc: 0.6125\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6371 - acc: 0.6125\n",
            "Score for fold 36: loss of 0.6370665431022644; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 37 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9474 - acc: 0.6250 - val_loss: 2.9374 - val_acc: 0.6250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9473 - acc: 0.6250 - val_loss: 2.9190 - val_acc: 0.6250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9437 - acc: 0.6250 - val_loss: 2.9310 - val_acc: 0.6250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9420 - acc: 0.6250 - val_loss: 2.9193 - val_acc: 0.6250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9383 - acc: 0.6250 - val_loss: 2.9166 - val_acc: 0.6250\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7404 - acc: 0.5875\n",
            "Score for fold 37: loss of 0.7403587102890015; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 38 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9854 - acc: 0.6219 - val_loss: 2.7241 - val_acc: 0.6500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9940 - acc: 0.6250 - val_loss: 2.7276 - val_acc: 0.7375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9882 - acc: 0.6250 - val_loss: 2.7152 - val_acc: 0.6500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9841 - acc: 0.6281 - val_loss: 2.7147 - val_acc: 0.6500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.9866 - acc: 0.6406 - val_loss: 2.7212 - val_acc: 0.7750\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.9054 - acc: 0.6375\n",
            "Score for fold 38: loss of 0.9054384231567383; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 39 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.8358 - acc: 0.6812 - val_loss: 3.2997 - val_acc: 0.5750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8361 - acc: 0.6562 - val_loss: 3.2997 - val_acc: 0.6000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8333 - acc: 0.6719 - val_loss: 3.3006 - val_acc: 0.6000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8317 - acc: 0.6969 - val_loss: 3.3040 - val_acc: 0.5875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.8330 - acc: 0.6906 - val_loss: 3.3089 - val_acc: 0.5875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8131 - acc: 0.5250\n",
            "Score for fold 39: loss of 0.8131016492843628; acc of 52.49999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 40 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0425 - acc: 0.7750 - val_loss: 2.6329 - val_acc: 0.6625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0078 - acc: 0.7500 - val_loss: 2.7486 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.1205 - acc: 0.7969 - val_loss: 2.6161 - val_acc: 0.8125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 3.0245 - acc: 0.8062 - val_loss: 2.6251 - val_acc: 0.7500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.0144 - acc: 0.7031 - val_loss: 2.6175 - val_acc: 0.8125\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5133 - acc: 0.6500\n",
            "Score for fold 40: loss of 0.5133044123649597; acc of 64.99999761581421%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_148\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_321 (Dense)           (None, 512)               12800     \n",
            "                                                                 \n",
            " dense_322 (Dense)           (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,826\n",
            "Trainable params: 13,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 41 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 0.4159 - acc: 0.8562 - val_loss: 0.2105 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1544 - acc: 0.9312 - val_loss: 0.1650 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1216 - acc: 0.9438 - val_loss: 0.1415 - val_acc: 0.9125\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1410 - acc: 0.9438 - val_loss: 0.3152 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1478 - acc: 0.9531 - val_loss: 0.1103 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8604 - acc: 0.6125\n",
            "Score for fold 41: loss of 0.8603929281234741; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 42 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1089 - acc: 0.9625 - val_loss: 0.0915 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1026 - acc: 0.9594 - val_loss: 0.0868 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0998 - acc: 0.9594 - val_loss: 0.0807 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6308 - acc: 0.9250 - val_loss: 1.8319 - val_acc: 0.8500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8595 - acc: 0.9062 - val_loss: 0.7999 - val_acc: 0.9000\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5171 - acc: 0.6500\n",
            "Score for fold 42: loss of 0.5171343088150024; acc of 64.99999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 43 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.3044 - acc: 0.9531 - val_loss: 0.0960 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1156 - acc: 0.9656 - val_loss: 0.0634 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0857 - acc: 0.9688 - val_loss: 0.0661 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0817 - acc: 0.9688 - val_loss: 0.0658 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0767 - acc: 0.9719 - val_loss: 0.0579 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7277 - acc: 0.5750\n",
            "Score for fold 43: loss of 0.7277466654777527; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 44 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0560 - acc: 0.9625 - val_loss: 0.1543 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0503 - acc: 0.9781 - val_loss: 0.1541 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0474 - acc: 0.9812 - val_loss: 0.1559 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0464 - acc: 0.9812 - val_loss: 0.2393 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0425 - acc: 0.9781 - val_loss: 0.1628 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7347 - acc: 0.6125\n",
            "Score for fold 44: loss of 0.7347429394721985; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 45 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0942 - acc: 0.9781 - val_loss: 0.0247 - val_acc: 0.9875\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0920 - acc: 0.9812 - val_loss: 0.0238 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0899 - acc: 0.9844 - val_loss: 0.0234 - val_acc: 0.9875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0896 - acc: 0.9812 - val_loss: 0.1173 - val_acc: 0.9875\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0894 - acc: 0.9750 - val_loss: 0.0233 - val_acc: 0.9875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7693 - acc: 0.5625\n",
            "Score for fold 45: loss of 0.7692528963088989; acc of 56.25%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "g=0\n",
        "figure, axis = plt.subplots(2,2)\n",
        "\n",
        "\n",
        " \n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "for i in ['' ,'relu', 'tanh' , 'sigmoid' ] :\n",
        "  loss_test_per_fold=[]\n",
        "  acc_test_per_fold=[]\n",
        "  acc_per_fold=[]\n",
        "  loss_per_fold=[]\n",
        "  if i=='':\n",
        "    model1= Sequential()\n",
        "    model1.add(Dense(40 , input_dim=24))\n",
        "    model1.add(Dense(2))\n",
        "  if i!= '':  \n",
        "    model1 = Sequential()\n",
        "    model1.add(Dense(40,  activation=i , input_dim=24))\n",
        "    model1.add(Dense(2, activation=i) )\n",
        "\n",
        "  model1.compile(optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc'])\n",
        "  model1.summary() \n",
        "  for train, test in kfold.split(X , Y): \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    Y_test = to_categorical(Y_test)\n",
        "    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    \n",
        "     \n",
        "    \n",
        "    # Fit data to model\n",
        "    history = model1.fit(X_train, Y_train ,\n",
        "                validation_data=(X_test , Y_test) ,       \n",
        "                batch_size=10,\n",
        "                epochs=5\n",
        "                )\n",
        "    acc= history.history['acc']\n",
        "    loss=history.history['loss']\n",
        "    val_acc=history.history['val_acc']\n",
        "    val_loss=history.history['val_loss']\n",
        "    scores = model.evaluate(X_test,Y_test )\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    loss_test_per_fold.append(val_loss)\n",
        "    acc_test_per_fold.append(val_acc)\n",
        "    acc_per_fold.append(acc)\n",
        "    loss_per_fold.append(loss)\n",
        "\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "  \n",
        "  acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "  acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "  axis[g//2 , g%2].plot(acc_test, 'r^-', label='test accuracy')\n",
        "  axis[g//2 , g%2].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "\n",
        "  acc_per_fold= np.array(acc_per_fold)\n",
        "  acc=np.sum(acc_per_fold , axis=0)/5\n",
        "  axis[g//2 , g%2].plot(acc, 'bo-', label='Train acc')\n",
        "  axis[g//2 , g%2].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()  \n",
        "\n",
        "  g=g+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TVQ08whETu5_",
        "outputId": "22513085-f8e4-4963-e1ca-f251bd768cbf"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_167\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_359 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_360 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 1.0700 - acc: 0.6687 - val_loss: 0.4674 - val_acc: 0.7500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4897 - acc: 0.7906 - val_loss: 0.2975 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3641 - acc: 0.9156 - val_loss: 0.2374 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3103 - acc: 0.9250 - val_loss: 0.1868 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2775 - acc: 0.9250 - val_loss: 0.1520 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8527 - acc: 0.6000\n",
            "Score for fold 1: loss of 0.8527294397354126; acc of 60.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2429 - acc: 0.9281 - val_loss: 0.0995 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1734 - acc: 0.9438 - val_loss: 0.0867 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1611 - acc: 0.9344 - val_loss: 0.1104 - val_acc: 0.9875\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1559 - acc: 0.9438 - val_loss: 0.0780 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1167 - acc: 0.9469 - val_loss: 0.0647 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7588 - acc: 0.6250\n",
            "Score for fold 2: loss of 0.7587941288948059; acc of 62.5%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0985 - acc: 0.9469 - val_loss: 0.1020 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0929 - acc: 0.9531 - val_loss: 0.0962 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0895 - acc: 0.9531 - val_loss: 0.0934 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0866 - acc: 0.9563 - val_loss: 0.0956 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0827 - acc: 0.9531 - val_loss: 0.0892 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5343 - acc: 0.6625\n",
            "Score for fold 3: loss of 0.5343453884124756; acc of 66.25000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0744 - acc: 0.9625 - val_loss: 0.1062 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0707 - acc: 0.9688 - val_loss: 0.1083 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0697 - acc: 0.9656 - val_loss: 0.2574 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0663 - acc: 0.9625 - val_loss: 0.2551 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0623 - acc: 0.9656 - val_loss: 0.2543 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6780 - acc: 0.6000\n",
            "Score for fold 4: loss of 0.6780436635017395; acc of 60.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1014 - acc: 0.9781 - val_loss: 0.0863 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1003 - acc: 0.9750 - val_loss: 0.0792 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0987 - acc: 0.9688 - val_loss: 0.0762 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0966 - acc: 0.9781 - val_loss: 0.0739 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0926 - acc: 0.9844 - val_loss: 0.0745 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7854 - acc: 0.5250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 0.7853571772575378; acc of 52.49999761581421%\n",
            "Model: \"sequential_168\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_361 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_362 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 1.4629 - acc: 0.6906 - val_loss: 0.5833 - val_acc: 0.6500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5270 - acc: 0.8469 - val_loss: 0.5038 - val_acc: 0.9000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4264 - acc: 0.9531 - val_loss: 0.4410 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3587 - acc: 0.9406 - val_loss: 0.3816 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3025 - acc: 0.9438 - val_loss: 0.3261 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6933 - acc: 0.5250\n",
            "Score for fold 6: loss of 0.6932897567749023; acc of 52.49999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.2669 - acc: 0.9531 - val_loss: 0.2379 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2310 - acc: 0.9500 - val_loss: 0.2073 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1988 - acc: 0.9469 - val_loss: 0.1882 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1759 - acc: 0.9531 - val_loss: 0.1704 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1586 - acc: 0.9500 - val_loss: 0.1543 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6920 - acc: 0.5875\n",
            "Score for fold 7: loss of 0.6920264959335327; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1511 - acc: 0.9406 - val_loss: 0.1023 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1370 - acc: 0.9406 - val_loss: 0.0897 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1248 - acc: 0.9406 - val_loss: 0.0814 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1146 - acc: 0.9406 - val_loss: 0.0737 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1067 - acc: 0.9438 - val_loss: 0.0694 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8488 - acc: 0.6750\n",
            "Score for fold 8: loss of 0.8487867116928101; acc of 67.5000011920929%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9563 - val_loss: 0.0753 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0942 - acc: 0.9594 - val_loss: 0.0721 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0906 - acc: 0.9625 - val_loss: 0.0695 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0857 - acc: 0.9625 - val_loss: 0.0674 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0813 - acc: 0.9625 - val_loss: 0.0657 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7438 - acc: 0.6375\n",
            "Score for fold 9: loss of 0.7437697649002075; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0867 - acc: 0.9656 - val_loss: 0.0956 - val_acc: 0.9375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0755 - acc: 0.9750 - val_loss: 0.0829 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0704 - acc: 0.9719 - val_loss: 0.0817 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0708 - acc: 0.9719 - val_loss: 0.0790 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0682 - acc: 0.9719 - val_loss: 0.0773 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6314 - acc: 0.5875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 10: loss of 0.6313971281051636; acc of 58.74999761581421%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_169\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_363 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_364 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 11 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 8ms/step - loss: 0.6583 - acc: 0.7031 - val_loss: 0.4409 - val_acc: 0.8750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3958 - acc: 0.9094 - val_loss: 0.3429 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3015 - acc: 0.9312 - val_loss: 0.2820 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2454 - acc: 0.9125 - val_loss: 0.2315 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2056 - acc: 0.9219 - val_loss: 0.2076 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6438 - acc: 0.6750\n",
            "Score for fold 11: loss of 0.6438280940055847; acc of 67.5000011920929%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 12 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1941 - acc: 0.9312 - val_loss: 0.1357 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1698 - acc: 0.9344 - val_loss: 0.1199 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1574 - acc: 0.9344 - val_loss: 0.1074 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1465 - acc: 0.9469 - val_loss: 0.0990 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1369 - acc: 0.9438 - val_loss: 0.0970 - val_acc: 0.9875\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8368 - acc: 0.5500\n",
            "Score for fold 12: loss of 0.8368280529975891; acc of 55.000001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 13 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1311 - acc: 0.9500 - val_loss: 0.0993 - val_acc: 0.9750\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1315 - acc: 0.9406 - val_loss: 0.0916 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1226 - acc: 0.9500 - val_loss: 0.0828 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1149 - acc: 0.9531 - val_loss: 0.0820 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1289 - acc: 0.9469 - val_loss: 0.0901 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7053 - acc: 0.5625\n",
            "Score for fold 13: loss of 0.7053464651107788; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 14 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1239 - acc: 0.9406 - val_loss: 0.1579 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0990 - acc: 0.9594 - val_loss: 0.1357 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0939 - acc: 0.9594 - val_loss: 0.1289 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0901 - acc: 0.9594 - val_loss: 0.1271 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0872 - acc: 0.9656 - val_loss: 0.1215 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5649 - acc: 0.7125\n",
            "Score for fold 14: loss of 0.5648789405822754; acc of 71.24999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 15 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0812 - acc: 0.9625 - val_loss: 0.1345 - val_acc: 0.9375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0786 - acc: 0.9688 - val_loss: 0.1344 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0752 - acc: 0.9688 - val_loss: 0.1336 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0724 - acc: 0.9688 - val_loss: 0.1329 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0701 - acc: 0.9750 - val_loss: 0.1375 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8584 - acc: 0.5125\n",
            "Score for fold 15: loss of 0.8583882451057434; acc of 51.249998807907104%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_170\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_365 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_366 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 16 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 1s 7ms/step - loss: 0.6670 - acc: 0.6438 - val_loss: 0.6878 - val_acc: 0.5500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6353 - acc: 0.6438 - val_loss: 0.6671 - val_acc: 0.5500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6088 - acc: 0.6438 - val_loss: 0.6336 - val_acc: 0.5500\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5820 - acc: 0.6438 - val_loss: 0.6004 - val_acc: 0.5500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.5534 - acc: 0.6438 - val_loss: 0.5673 - val_acc: 0.5500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7744 - acc: 0.5375\n",
            "Score for fold 16: loss of 0.7744323015213013; acc of 53.75000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 17 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.5246 - acc: 0.6281 - val_loss: 0.5207 - val_acc: 0.6125\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4905 - acc: 0.7188 - val_loss: 0.4870 - val_acc: 0.9750\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4564 - acc: 0.9563 - val_loss: 0.4542 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4227 - acc: 0.9781 - val_loss: 0.4237 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3913 - acc: 0.9625 - val_loss: 0.3951 - val_acc: 0.9125\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.8817 - acc: 0.6000\n",
            "Score for fold 17: loss of 0.8816898465156555; acc of 60.00000238418579%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 18 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.3657 - acc: 0.9312 - val_loss: 0.3560 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3391 - acc: 0.9531 - val_loss: 0.3303 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3157 - acc: 0.9438 - val_loss: 0.3099 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2963 - acc: 0.9406 - val_loss: 0.2889 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2783 - acc: 0.9375 - val_loss: 0.2701 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.8158 - acc: 0.5625\n",
            "Score for fold 18: loss of 0.8158464431762695; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 19 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2598 - acc: 0.9438 - val_loss: 0.2683 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2449 - acc: 0.9438 - val_loss: 0.2551 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2325 - acc: 0.9375 - val_loss: 0.2444 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2207 - acc: 0.9438 - val_loss: 0.2343 - val_acc: 0.9250\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.2100 - acc: 0.9406 - val_loss: 0.2251 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5653 - acc: 0.6125\n",
            "Score for fold 19: loss of 0.5652636289596558; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 20 ...\n",
            "Epoch 1/5\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.2077 - acc: 0.9438 - val_loss: 0.1939 - val_acc: 0.9375\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1980 - acc: 0.9438 - val_loss: 0.1886 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1904 - acc: 0.9438 - val_loss: 0.1822 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1834 - acc: 0.9438 - val_loss: 0.1769 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1768 - acc: 0.9438 - val_loss: 0.1732 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5720 - acc: 0.7000\n",
            "Score for fold 20: loss of 0.5720375776290894; acc of 69.9999988079071%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "g=0\n",
        "figure, axis = plt.subplots(2,2)\n",
        "\n",
        "\n",
        " \n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "for i in [4 ,  32 , 64] :\n",
        "  loss_test_per_fold=[]\n",
        "  acc_test_per_fold=[]\n",
        "  acc_per_fold=[]\n",
        "  loss_per_fold=[]\n",
        "\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(40,  activation=\"relu\" , input_dim=24))\n",
        "  model1.add(Dense(2))\n",
        "\n",
        "  model1.compile(optimizer='adam',\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['acc'])\n",
        "  model1.summary() \n",
        "  for train, test in kfold.split(X , Y): \n",
        "    X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "    Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "    Y_train = to_categorical(Y_train)\n",
        "    Y_test = to_categorical(Y_test)\n",
        "    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "    \n",
        "     \n",
        "    \n",
        "    # Fit data to model\n",
        "    history = model1.fit(X_train, Y_train ,\n",
        "                validation_data=(X_test , Y_test) ,       \n",
        "                batch_size=i,\n",
        "                epochs=5\n",
        "                )\n",
        "    acc= history.history['acc']\n",
        "    loss=history.history['loss']\n",
        "    val_acc=history.history['val_acc']\n",
        "    val_loss=history.history['val_loss']\n",
        "    scores = model.evaluate(X_test,Y_test )\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    loss_test_per_fold.append(val_loss)\n",
        "    acc_test_per_fold.append(val_acc)\n",
        "    acc_per_fold.append(acc)\n",
        "    loss_per_fold.append(loss)\n",
        "\n",
        "\n",
        "    fold_no = fold_no + 1\n",
        "  \n",
        "  acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "  acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "  axis[g//2 , g%2].plot(acc_test, 'r^-', label='test accuracy')\n",
        "  axis[g//2 , g%2].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "\n",
        "  acc_per_fold= np.array(acc_per_fold)\n",
        "  acc=np.sum(acc_per_fold , axis=0)/5\n",
        "  axis[g//2 , g%2].plot(acc, 'bo-', label='Train acc')\n",
        "  axis[g//2 , g%2].set_title(f\"{i}\")\n",
        "  plt.grid()\n",
        "  plt.legend()  \n",
        "\n",
        "  g=g+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HwGQ3ocZX_Mc",
        "outputId": "c9d0766f-b723-4cea-a7c0-b5ed9ec29905"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_174\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_373 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_374 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 1s 4ms/step - loss: 3.2259 - acc: 0.5375 - val_loss: 0.7743 - val_acc: 0.7500\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.5028 - acc: 0.8938 - val_loss: 0.4539 - val_acc: 0.8875\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3339 - acc: 0.9438 - val_loss: 0.3523 - val_acc: 0.9000\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.2483 - acc: 0.9406 - val_loss: 0.2816 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.1915 - acc: 0.9469 - val_loss: 0.2406 - val_acc: 0.9125\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7936 - acc: 0.5625\n",
            "Score for fold 1: loss of 0.7935692071914673; acc of 56.25%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1958 - acc: 0.9531 - val_loss: 0.1982 - val_acc: 0.9125\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1567 - acc: 0.9500 - val_loss: 0.1847 - val_acc: 0.9125\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1192 - acc: 0.9563 - val_loss: 0.1610 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1037 - acc: 0.9531 - val_loss: 0.1286 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0977 - acc: 0.9563 - val_loss: 0.1190 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7195 - acc: 0.6500\n",
            "Score for fold 2: loss of 0.7195048332214355; acc of 64.99999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1039 - acc: 0.9563 - val_loss: 0.0736 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5382 - acc: 0.9250 - val_loss: 0.6380 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.5611 - acc: 0.9156 - val_loss: 0.2772 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2209 - acc: 0.9563 - val_loss: 0.0873 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1079 - acc: 0.9594 - val_loss: 0.0821 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6056 - acc: 0.5500\n",
            "Score for fold 3: loss of 0.6056097745895386; acc of 55.000001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1065 - acc: 0.9531 - val_loss: 0.0657 - val_acc: 0.9875\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.1004 - acc: 0.9531 - val_loss: 0.0611 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.0955 - acc: 0.9563 - val_loss: 0.0711 - val_acc: 0.9750\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0907 - acc: 0.9594 - val_loss: 0.0738 - val_acc: 0.9750\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.0869 - acc: 0.9625 - val_loss: 0.1455 - val_acc: 0.9750\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8187 - acc: 0.5875\n",
            "Score for fold 4: loss of 0.8187143206596375; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1089 - acc: 0.9563 - val_loss: 0.0485 - val_acc: 0.9875\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1059 - acc: 0.9563 - val_loss: 0.0466 - val_acc: 0.9875\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1034 - acc: 0.9594 - val_loss: 0.0448 - val_acc: 0.9875\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1014 - acc: 0.9594 - val_loss: 0.0444 - val_acc: 0.9875\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 0s 3ms/step - loss: 0.1000 - acc: 0.9625 - val_loss: 0.0424 - val_acc: 0.9875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6719 - acc: 0.6625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 5: loss of 0.671871542930603; acc of 66.25000238418579%\n",
            "Model: \"sequential_175\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_375 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_376 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 1s 22ms/step - loss: 0.9738 - acc: 0.3562 - val_loss: 0.7220 - val_acc: 0.5625\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.6823 - acc: 0.5312 - val_loss: 0.6023 - val_acc: 0.6250\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.5792 - acc: 0.5938 - val_loss: 0.5186 - val_acc: 0.6500\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4978 - acc: 0.6875 - val_loss: 0.4494 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.4277 - acc: 0.9312 - val_loss: 0.3892 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7626 - acc: 0.5875\n",
            "Score for fold 6: loss of 0.762622058391571; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3683 - acc: 0.9406 - val_loss: 0.3199 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.3193 - acc: 0.9438 - val_loss: 0.2680 - val_acc: 0.9625\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2788 - acc: 0.9438 - val_loss: 0.2284 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.2474 - acc: 0.9344 - val_loss: 0.1953 - val_acc: 0.9625\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.2202 - acc: 0.9344 - val_loss: 0.1672 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7093 - acc: 0.6250\n",
            "Score for fold 7: loss of 0.7092782258987427; acc of 62.5%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1880 - acc: 0.9375 - val_loss: 0.1831 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1679 - acc: 0.9375 - val_loss: 0.1642 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1504 - acc: 0.9375 - val_loss: 0.1482 - val_acc: 0.9500\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.1374 - acc: 0.9375 - val_loss: 0.1350 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1254 - acc: 0.9406 - val_loss: 0.1256 - val_acc: 0.9500\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6064 - acc: 0.6125\n",
            "Score for fold 8: loss of 0.6063880920410156; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1099 - acc: 0.9500 - val_loss: 0.1431 - val_acc: 0.9250\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.1019 - acc: 0.9500 - val_loss: 0.1350 - val_acc: 0.9250\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0971 - acc: 0.9500 - val_loss: 0.1313 - val_acc: 0.9250\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0927 - acc: 0.9563 - val_loss: 0.1247 - val_acc: 0.9250\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 0.0886 - acc: 0.9563 - val_loss: 0.1239 - val_acc: 0.9250\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7784 - acc: 0.5750\n",
            "Score for fold 9: loss of 0.7784252166748047; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/5\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0906 - acc: 0.9531 - val_loss: 0.1107 - val_acc: 0.9625\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0851 - acc: 0.9563 - val_loss: 0.1042 - val_acc: 0.9500\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 6ms/step - loss: 0.0814 - acc: 0.9563 - val_loss: 0.1144 - val_acc: 0.9625\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 0.0777 - acc: 0.9594 - val_loss: 0.1093 - val_acc: 0.9500\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0744 - acc: 0.9625 - val_loss: 0.1892 - val_acc: 0.9625\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7526 - acc: 0.6125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 10: loss of 0.7525561451911926; acc of 61.250001192092896%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_176\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_377 (Dense)           (None, 40)                1000      \n",
            "                                                                 \n",
            " dense_378 (Dense)           (None, 2)                 82        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,082\n",
            "Trainable params: 1,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 11 ...\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 49ms/step - loss: 1.7150 - acc: 0.4812 - val_loss: 1.0962 - val_acc: 0.5375\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.9931 - acc: 0.3938 - val_loss: 0.8928 - val_acc: 0.3500\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7770 - acc: 0.3906 - val_loss: 0.7396 - val_acc: 0.4250\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.6937 - acc: 0.4656 - val_loss: 0.6905 - val_acc: 0.4500\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.6476 - acc: 0.5219 - val_loss: 0.6553 - val_acc: 0.4625\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6183 - acc: 0.5750\n",
            "Score for fold 11: loss of 0.6183107495307922; acc of 57.499998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 12 ...\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.6248 - acc: 0.5250 - val_loss: 0.5757 - val_acc: 0.5750\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5924 - acc: 0.5406 - val_loss: 0.5451 - val_acc: 0.6125\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.5619 - acc: 0.5500 - val_loss: 0.5164 - val_acc: 0.6125\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5332 - acc: 0.5594 - val_loss: 0.4896 - val_acc: 0.6250\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.5060 - acc: 0.6094 - val_loss: 0.4652 - val_acc: 0.6875\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9523 - acc: 0.5875\n",
            "Score for fold 12: loss of 0.9523192644119263; acc of 58.74999761581421%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 13 ...\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.4702 - acc: 0.7188 - val_loss: 0.4873 - val_acc: 0.8125\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.4448 - acc: 0.8594 - val_loss: 0.4673 - val_acc: 0.8875\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 9ms/step - loss: 0.4221 - acc: 0.9094 - val_loss: 0.4487 - val_acc: 0.9000\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3996 - acc: 0.9281 - val_loss: 0.4304 - val_acc: 0.8875\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3797 - acc: 0.9156 - val_loss: 0.4126 - val_acc: 0.8875\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.7731 - acc: 0.6125\n",
            "Score for fold 13: loss of 0.7731043100357056; acc of 61.250001192092896%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 14 ...\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 20ms/step - loss: 0.3705 - acc: 0.9094 - val_loss: 0.3466 - val_acc: 0.9500\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3510 - acc: 0.9094 - val_loss: 0.3274 - val_acc: 0.9375\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3323 - acc: 0.9187 - val_loss: 0.3085 - val_acc: 0.9375\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.3136 - acc: 0.9219 - val_loss: 0.2908 - val_acc: 0.9375\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2974 - acc: 0.9281 - val_loss: 0.2762 - val_acc: 0.9375\n",
            "3/3 [==============================] - 0s 3ms/step - loss: 0.6911 - acc: 0.6375\n",
            "Score for fold 14: loss of 0.691118597984314; acc of 63.749998807907104%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 15 ...\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.2781 - acc: 0.9406 - val_loss: 0.2857 - val_acc: 0.9000\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2636 - acc: 0.9438 - val_loss: 0.2744 - val_acc: 0.9000\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2501 - acc: 0.9438 - val_loss: 0.2638 - val_acc: 0.9000\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2365 - acc: 0.9438 - val_loss: 0.2529 - val_acc: 0.9125\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.2233 - acc: 0.9438 - val_loss: 0.2425 - val_acc: 0.9000\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5744 - acc: 0.6000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n",
            "WARNING:matplotlib.legend:No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 15: loss of 0.5744169354438782; acc of 60.00000238418579%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "loss_test_per_fold=[]\n",
        "acc_test_per_fold=[]\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "fold_no = 1\n",
        "X = (X - X.min()) / (X.max() - X.min()) \n",
        "\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(32,  activation=\"relu\" , input_dim=24))\n",
        "model1.add(Dense(64,  activation=\"relu\" ))\n",
        "model1.add(Dense(32,  activation=\"relu\" ))\n",
        "model1.add(Dense(2))\n",
        "\n",
        "model1.compile(optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['acc'])\n",
        "model1.summary() \n",
        "for train, test in kfold.split(X , Y): \n",
        "  X_train, X_test = X.iloc[train], X.iloc[test]\n",
        "  Y_train, Y_test = Y.iloc[train], Y.iloc[test]\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_test = to_categorical(Y_test)\n",
        "  #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "  \n",
        "    \n",
        "  \n",
        "  # Fit data to model\n",
        "  history = model1.fit(X_train, Y_train ,\n",
        "              validation_data=(X_test , Y_test) ,       \n",
        "              batch_size=10,\n",
        "              epochs=10\n",
        "              )\n",
        "  acc= history.history['acc']\n",
        "  loss=history.history['loss']\n",
        "  val_acc=history.history['val_acc']\n",
        "  val_loss=history.history['val_loss']\n",
        "  #scores = model.evaluate(X_test,Y_test )\n",
        "  #print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  loss_test_per_fold.append(val_loss)\n",
        "  acc_test_per_fold.append(val_acc)\n",
        "  acc_per_fold.append(acc)\n",
        "  loss_per_fold.append(loss)\n",
        "\n",
        "\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "acc_test_per_fold= np.array(acc_test_per_fold)\n",
        "acc_test=np.sum(acc_test_per_fold , axis=0)/5\n",
        "plt.plot(acc_test, 'r^-', label='test accuracy')\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "acc_per_fold= np.array(acc_per_fold)\n",
        "acc=np.sum(acc_per_fold , axis=0)/5\n",
        "\n",
        "plt.plot(acc, 'bo-', label='Train acc')\n",
        "\n",
        "plt.grid()\n",
        "plt.legend()  \n",
        "print(f'accuracy test ==>{acc_test[-1]*100} %')\n",
        "#print(f'accuracy  ==>{acc[-1]*100} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UuKKYRFVNZvP",
        "outputId": "715dd2b5-b5b6-4a36-bd0e-ee6b9cdbe20a"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_208\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_493 (Dense)           (None, 32)                800       \n",
            "                                                                 \n",
            " dense_494 (Dense)           (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_495 (Dense)           (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_496 (Dense)           (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,058\n",
            "Trainable params: 5,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6043 - acc: 0.7344 - val_loss: 0.3353 - val_acc: 0.9500\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1977 - acc: 0.9438 - val_loss: 0.1370 - val_acc: 0.9500\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1340 - acc: 0.9531 - val_loss: 0.0970 - val_acc: 0.9750\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0862 - acc: 0.9594 - val_loss: 0.1232 - val_acc: 0.9875\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1181 - acc: 0.9719 - val_loss: 0.0718 - val_acc: 0.9875\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1025 - acc: 0.9750 - val_loss: 0.0719 - val_acc: 0.9875\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0995 - acc: 0.9750 - val_loss: 0.0578 - val_acc: 0.9875\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0848 - acc: 0.9844 - val_loss: 0.0683 - val_acc: 0.9875\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.1188 - acc: 0.9812 - val_loss: 0.0561 - val_acc: 0.9875\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.1338 - acc: 0.9563 - val_loss: 0.0670 - val_acc: 0.9875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.1597 - acc: 0.9656 - val_loss: 0.0629 - val_acc: 0.9625\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0719 - acc: 0.9937 - val_loss: 0.0592 - val_acc: 0.9500\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0683 - acc: 0.9937 - val_loss: 0.0652 - val_acc: 0.9625\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0651 - acc: 0.9937 - val_loss: 0.0675 - val_acc: 0.9500\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0658 - acc: 0.9937 - val_loss: 0.0883 - val_acc: 0.9625\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0629 - acc: 0.9969 - val_loss: 0.0752 - val_acc: 0.9625\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0599 - acc: 0.9969 - val_loss: 0.0862 - val_acc: 0.9625\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0610 - acc: 0.9937 - val_loss: 0.1668 - val_acc: 0.9500\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0595 - acc: 0.9969 - val_loss: 0.1845 - val_acc: 0.9625\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - acc: 0.9937 - val_loss: 0.2691 - val_acc: 0.9625\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0405 - acc: 0.9812 - val_loss: 0.6768 - val_acc: 0.9250\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.4481 - acc: 0.9187 - val_loss: 0.5387 - val_acc: 0.9375\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.3852 - acc: 0.9438 - val_loss: 0.2428 - val_acc: 0.9625\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0811 - acc: 0.9781 - val_loss: 0.2133 - val_acc: 0.9875\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0293 - acc: 0.9906 - val_loss: 0.2096 - val_acc: 0.9875\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0267 - acc: 0.9937 - val_loss: 0.2096 - val_acc: 0.9875\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0246 - acc: 0.9906 - val_loss: 0.2083 - val_acc: 0.9875\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0236 - acc: 0.9906 - val_loss: 0.2062 - val_acc: 0.9875\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0223 - acc: 0.9875 - val_loss: 0.2041 - val_acc: 0.9875\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0211 - acc: 0.9906 - val_loss: 0.2057 - val_acc: 0.9875\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0698 - acc: 0.9844 - val_loss: 0.0064 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0700 - acc: 0.9875 - val_loss: 0.0060 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0683 - acc: 0.9875 - val_loss: 0.0063 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0690 - acc: 0.9875 - val_loss: 0.0059 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0666 - acc: 0.9844 - val_loss: 0.0062 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0671 - acc: 0.9844 - val_loss: 0.0063 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0693 - acc: 0.9875 - val_loss: 0.0070 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0661 - acc: 0.9875 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0663 - acc: 0.9906 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0657 - acc: 0.9875 - val_loss: 0.0057 - val_acc: 1.0000\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.0665 - acc: 0.9875 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0659 - acc: 0.9937 - val_loss: 1.4498e-04 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0650 - acc: 0.9906 - val_loss: 9.4827e-04 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0645 - acc: 0.9906 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0661 - acc: 0.9906 - val_loss: 5.2009e-04 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0647 - acc: 0.9906 - val_loss: 3.7955e-05 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - acc: 0.9906 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.0649 - acc: 0.9906 - val_loss: 1.3009e-04 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0627 - acc: 0.9906 - val_loss: 1.1154e-04 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.0664 - acc: 0.9906 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
            "accuracy test ==>98.75 %\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}